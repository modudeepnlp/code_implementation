{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "  해당 레퍼런스를 자세히 뜯어보는 코드입니다. \n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n",
    "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "              https://github.com/JayParks/transformer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps (padding)\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Transformer Parameters\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "print(src_vocab)\n",
    "print(src_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'P', 1: 'i', 2: 'want', 3: 'a', 4: 'beer', 5: 'S', 6: 'E'}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'S' : 5, 'E' : 6}\n",
    "number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "print(number_dict)\n",
    "print(tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model = Transformer()  \n",
    "Encoder, Decoder 객체를 각각 만들어서 연결!  \n",
    "\n",
    "1) Encoder 클래스 이해하기  \n",
    "2) Decoder 클래스 이해하기  \n",
    "\n",
    "<흐름>\n",
    "0. batch 생성: enc_inputs, dec_inputs, target_batch\n",
    "1. encoder(enc_inputs) -> enc_outputs, enc_self_attns\n",
    "2. decoder(dec_inputs, enc_inputs, enc_outputs) -> dec_outputs, dec_self_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0) batch 생성\n",
    "```python\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return Variable(torch.LongTensor(input_batch)), Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ich', 'mochte', 'ein', 'bier', 'P']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[src_vocab[n] for n in sentences[0].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 0]]\n",
      "tensor([[1, 2, 3, 4, 0]])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "print(input_batch)\n",
    "\n",
    "enc_inputs =  Variable(torch.LongTensor(input_batch))\n",
    "print(enc_inputs)\n",
    "print(enc_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 1, 2, 3, 4]]) tensor([[1, 2, 3, 4, 6]])\n",
      "torch.Size([1, 5]) torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "dec_inputs, target_batch = Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "print(dec_inputs, target_batch)\n",
    "print(dec_inputs.shape, target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Encoder 클래스\n",
    "```python\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_vocab_size, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "```\n",
    "\n",
    "  \n",
    "  \n",
    "필요한 것\n",
    "* enc_inputs(o)\n",
    "* src_vocab_size(o), d_model\n",
    "* get_sinusoid_encoding_table\n",
    "\n",
    "* get_attn_pad_mask\n",
    "* EncoderLayer\n",
    "\n",
    "1.1) src_emb  \n",
    "1.2) pos_emb  \n",
    "1.3) enc_outputs  \n",
    "1.4) enc_self_attn_mask  \n",
    "1.5) enc_self_attns  \n",
    "\n",
    "-> enc_outputs, enc_self_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (잠시) 전체파일 구조보기\n",
    "\n",
    "<함수>  \n",
    "get_sinusoid_encoding_table  \n",
    "get_attn_pad_mask  \n",
    "get_attn_subsequent_mask  \n",
    "  \n",
    "<클래스>  \n",
    "ScaledDotProductAttention  \n",
    "MultiHeadAttention  \n",
    "PoswiseFeedForwardNet  \n",
    "EncoderLayer  \n",
    "DecoderLayer  \n",
    "Encoder  \n",
    "Decoder  \n",
    "Transformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(5, 512)\n"
     ]
    }
   ],
   "source": [
    "# 1.1) self.src_emb\n",
    "\n",
    "d_model = 512  # Embedding Size\n",
    "src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "print(src_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1280,  2.3881,  0.0602,  ..., -0.0226, -1.2408,  1.9292],\n",
      "        [-1.7390,  0.9872, -0.2620,  ..., -0.9140,  0.7316, -0.1670],\n",
      "        [ 0.1747, -1.1317, -0.5184,  ..., -0.7197, -0.0271, -0.4607],\n",
      "        [-0.8533,  1.5153, -1.2670,  ..., -0.0183,  0.2211,  0.0245],\n",
      "        [-1.8163,  1.3144, -2.3754,  ..., -1.1859,  0.4277, -0.6877]],\n",
      "       requires_grad=True)\n",
      "torch.Size([5, 512])\n",
      "{'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4}\n",
      "tensor([-1.2803e-01,  2.3881e+00,  6.0206e-02,  7.7500e-01, -4.1753e-01,\n",
      "        -4.4098e-01,  6.2275e-01, -1.8652e+00, -2.1891e-01, -1.3015e+00,\n",
      "        -9.3235e-01,  1.7837e-01, -7.0967e-01, -1.1873e+00, -8.4855e-01,\n",
      "         2.6412e-01,  7.9109e-01, -1.9532e+00, -3.9711e-01,  2.3889e-01,\n",
      "         1.3457e+00, -7.0276e-01, -6.2562e-01,  4.5948e-01,  2.2145e-01,\n",
      "        -1.2298e+00,  1.0109e+00,  4.2429e-01, -4.9430e-01,  7.1398e-01,\n",
      "        -1.6573e+00,  3.2635e-01,  1.4090e+00,  4.7146e-01,  1.0180e+00,\n",
      "         1.9441e-01, -2.5765e-03,  3.5068e-02, -2.3918e+00, -2.3341e-01,\n",
      "        -1.3065e+00, -3.8956e-01, -6.2915e-01,  9.1776e-01, -3.8642e-01,\n",
      "         4.1299e-01, -5.0148e-01, -4.0322e-02,  7.8030e-01, -2.1401e-01,\n",
      "         3.9499e-01,  1.1085e+00, -2.1638e+00, -3.1014e+00,  4.2050e-01,\n",
      "         1.3124e+00,  1.0908e+00, -5.2577e-01, -2.7341e-01,  2.0019e+00,\n",
      "        -5.9317e-01,  1.7730e+00, -6.0142e-01,  7.4847e-01,  8.3454e-01,\n",
      "        -2.8531e-01,  7.1674e-01,  9.1598e-01, -2.1985e+00,  2.7272e+00,\n",
      "        -1.2952e-01, -1.4278e-01, -4.3120e-02,  6.2127e-01, -3.8545e-01,\n",
      "         3.6959e-01,  1.1530e+00, -7.2308e-01, -4.8725e-01, -1.0533e+00,\n",
      "        -1.6666e-01, -5.8539e-01,  1.8964e+00,  3.2005e-01,  2.3811e-01,\n",
      "        -3.0405e-02, -2.4082e-01,  1.6958e+00,  2.6749e+00, -1.4594e+00,\n",
      "         2.7321e-01,  1.7012e+00, -3.3838e-01,  5.6172e-01, -5.5297e-01,\n",
      "         8.3051e-01,  1.8060e+00,  1.2870e+00,  9.3587e-01,  5.9671e-01,\n",
      "        -8.4250e-01, -2.3371e+00,  5.7979e-01,  1.0041e+00,  1.2038e+00,\n",
      "        -3.7540e-01, -3.0085e-01, -7.8068e-01, -1.0988e+00,  2.8154e-01,\n",
      "        -4.2960e-02,  1.6232e+00, -3.6396e-01, -6.3559e-01,  1.3239e+00,\n",
      "         6.5341e-01,  2.6556e-01, -5.4622e-01, -1.6059e+00, -1.3631e+00,\n",
      "        -2.6650e-01, -4.0481e-01,  5.2375e-01,  1.0453e+00,  4.9392e-02,\n",
      "        -5.9950e-02, -1.0419e-02,  1.4137e+00,  1.1088e-01,  9.6427e-01,\n",
      "         1.4993e+00, -1.0483e+00, -3.0391e-01, -2.0592e-01, -4.2642e-01,\n",
      "        -7.6240e-01,  8.9109e-01,  7.4676e-01,  1.8649e+00,  7.1279e-01,\n",
      "        -1.1797e+00,  6.9787e-01,  1.4538e+00, -1.3729e+00, -1.0533e+00,\n",
      "         8.6050e-01, -1.1396e+00, -8.5316e-01, -5.3818e-01,  5.0243e-01,\n",
      "        -7.9890e-01, -4.4724e-01,  2.8408e-01, -1.0695e-01, -2.8523e-01,\n",
      "         1.0662e+00, -5.1060e-01,  2.7290e-01,  9.2996e-01, -1.0087e+00,\n",
      "        -1.2566e+00, -5.4780e-01, -5.7818e-01, -1.7020e-01, -2.6098e-01,\n",
      "        -2.6728e+00, -5.1281e-01, -5.0042e-01,  9.5925e-01,  2.4475e+00,\n",
      "        -2.5458e-01, -8.8262e-01,  2.1876e-02, -3.8759e-01, -2.9954e-01,\n",
      "         1.2083e-01,  1.8050e+00,  1.0942e+00,  8.4741e-01,  1.1579e+00,\n",
      "         2.5229e-01,  7.7756e-01, -1.8286e+00, -1.8499e+00, -3.5804e+00,\n",
      "        -4.0756e-01, -9.1197e-01, -7.1402e-01, -1.6185e+00, -4.8628e-01,\n",
      "        -1.2690e+00,  1.8181e+00, -1.1387e+00,  2.8719e-01, -3.2713e-01,\n",
      "        -1.1743e+00, -1.5404e+00,  2.7811e-01, -1.8953e-02,  1.5049e-01,\n",
      "        -1.0872e+00, -1.5031e+00, -1.1666e+00,  4.8637e-01,  5.4720e-01,\n",
      "        -1.7555e+00, -1.0202e-01,  1.2470e+00,  1.9720e-01, -4.3800e-01,\n",
      "        -1.5726e-01,  4.7698e-01,  7.8156e-01,  6.7622e-01, -9.3242e-01,\n",
      "         5.6816e-01, -4.8769e-01, -2.2968e-01, -7.4653e-01,  1.5728e-02,\n",
      "         5.0132e-01, -2.4147e-01,  1.2359e+00, -5.3480e-01,  2.0262e-01,\n",
      "         4.8469e-01,  9.5006e-01, -9.4511e-02, -1.2026e+00, -9.7949e-01,\n",
      "        -5.7494e-01,  4.9100e-01,  9.1501e-02,  9.4787e-01,  1.1427e+00,\n",
      "        -1.8236e-01, -1.8040e+00,  8.7663e-01,  3.3247e-01, -7.1566e-01,\n",
      "         1.3711e+00, -1.5850e+00,  1.3594e+00,  5.9612e-01,  8.3443e-01,\n",
      "        -1.8301e+00, -2.4951e+00,  3.0245e-02, -7.5003e-02,  1.3327e+00,\n",
      "         2.5180e-01, -1.3114e+00, -1.5234e-01,  1.8355e+00, -1.4333e+00,\n",
      "         6.1117e-03, -3.0556e-01,  1.3632e+00,  4.5308e-01, -5.1186e-02,\n",
      "         5.0804e-01, -2.5543e-01,  9.6514e-02, -1.1641e+00, -1.7943e+00,\n",
      "         1.8486e-01, -4.7542e-01,  5.5587e-01, -4.8505e-01, -3.1413e-01,\n",
      "        -5.0713e-01, -3.9731e-01,  1.8198e+00,  7.3806e-02,  1.8014e-01,\n",
      "        -1.0447e+00,  1.4403e+00, -9.9874e-01, -3.5544e-02,  3.8505e-01,\n",
      "        -1.1910e+00,  3.7319e-01,  5.7634e-01,  1.2345e+00,  9.0787e-01,\n",
      "         7.8246e-01,  7.2009e-01, -1.2871e+00,  1.1494e+00, -6.9597e-01,\n",
      "         5.7962e-01, -1.0023e+00,  5.3541e-01,  2.8288e-01, -4.3052e-01,\n",
      "        -1.6707e+00, -4.7303e-01,  1.1908e+00,  3.0797e-01,  1.4787e+00,\n",
      "        -5.1993e-01, -1.1577e+00,  1.3058e+00,  1.2259e+00, -1.0572e+00,\n",
      "        -6.1291e-01, -8.1992e-01, -2.5597e-01, -1.2266e+00,  4.3845e-01,\n",
      "         1.5693e-01,  7.1441e-01,  2.5513e+00,  5.1940e-02,  3.4878e-01,\n",
      "        -1.4579e+00,  1.0457e-01,  7.0584e-01, -1.7135e-01, -8.6426e-01,\n",
      "        -7.9607e-01,  4.4299e-01, -7.6142e-01, -5.1536e-01, -7.5218e-01,\n",
      "        -2.6420e-01, -8.0367e-01,  2.5609e-01, -4.7520e-01,  3.9280e-01,\n",
      "         2.4606e-01, -1.3797e-01, -9.8062e-02,  1.2874e+00,  2.5812e+00,\n",
      "        -2.4716e-01,  1.0034e+00, -2.0307e-02,  1.5206e+00,  1.9909e-01,\n",
      "        -1.5532e-01,  3.0177e-01,  1.8246e-01,  1.7837e-01,  1.0953e+00,\n",
      "         1.7272e-01, -1.4453e-01,  5.3239e-01, -8.0193e-01, -7.1833e-01,\n",
      "         2.0982e+00,  3.8389e-01, -1.5924e+00,  1.0155e+00,  9.8984e-01,\n",
      "        -1.6100e+00,  1.1070e+00,  8.2722e-01, -9.9831e-01, -2.3701e-01,\n",
      "         2.4926e-01, -3.1334e-02,  7.2649e-01, -5.6949e-01,  1.7541e+00,\n",
      "         4.3846e-01,  5.0783e-01, -6.6894e-01,  1.9204e-01, -1.0320e+00,\n",
      "        -5.9820e-01, -6.4862e-01,  9.3959e-01,  9.3388e-02, -2.6794e+00,\n",
      "         3.9987e-01,  1.0706e+00,  1.1740e+00,  1.0655e+00, -1.0506e+00,\n",
      "        -1.0836e-01, -9.4739e-01, -6.7644e-01, -1.9206e-01,  4.1884e-02,\n",
      "         1.5382e+00,  5.9507e-01,  3.7638e-01, -8.6672e-01,  3.1209e-01,\n",
      "         4.9515e-01,  8.0031e-01, -2.9660e-01,  4.9330e-01,  3.2947e-01,\n",
      "        -2.1604e+00, -5.6465e-01,  1.3404e+00,  1.5476e+00,  1.1879e+00,\n",
      "         1.2885e+00, -2.4455e+00, -2.9996e-01,  6.9726e-01, -9.9913e-01,\n",
      "        -1.3332e+00, -1.2518e+00,  1.2947e+00, -6.8582e-01, -1.3910e+00,\n",
      "         4.3335e-01, -1.4085e+00, -1.7392e+00, -4.5642e-02, -1.6028e+00,\n",
      "        -1.3566e-01, -4.5094e-02, -5.4744e-01, -4.9048e-01,  4.7302e-01,\n",
      "        -2.2347e-01, -8.9888e-01, -7.7407e-01,  9.9991e-01,  7.8055e-01,\n",
      "        -4.5725e-01, -1.2755e+00, -1.3887e+00,  2.5101e-02, -4.0249e-01,\n",
      "         1.3354e+00, -3.4056e-02, -3.9207e-02,  5.3564e-01, -6.2248e-01,\n",
      "         8.1667e-01, -7.0760e-01,  1.3511e+00, -7.7394e-01, -5.0271e-01,\n",
      "        -4.2511e-02, -8.5983e-02,  1.6790e+00,  1.5539e+00, -1.2118e+00,\n",
      "        -5.1581e-01,  1.8725e-01, -2.1252e+00,  9.9051e-01,  6.7742e-01,\n",
      "         1.2383e+00,  2.2503e+00, -6.1798e-03,  5.1439e-01, -2.1501e-01,\n",
      "         8.9284e-01,  3.5539e-01,  3.8972e-01,  2.6662e-02, -1.5792e+00,\n",
      "        -9.4603e-01, -1.0379e+00, -6.4132e-02,  1.5150e+00,  2.2457e+00,\n",
      "        -1.3316e-01,  1.2412e+00, -7.8840e-01,  1.7354e+00,  8.6621e-01,\n",
      "         1.6197e-01, -1.7354e+00,  1.3707e+00,  8.3502e-01, -1.3322e+00,\n",
      "         1.2697e+00, -1.6747e+00, -8.6290e-01, -8.3071e-02, -9.8190e-01,\n",
      "        -4.6957e-01,  6.7195e-01, -1.3924e+00, -1.3553e+00, -4.9157e-02,\n",
      "         2.3395e-01,  5.3267e-01, -3.0085e-01,  9.4052e-01,  8.8921e-01,\n",
      "         2.6592e+00,  1.1105e+00,  1.5003e+00, -1.8092e+00,  1.2023e+00,\n",
      "        -6.0082e-01, -3.1683e-01,  1.6158e+00,  6.6983e-01, -1.6047e-01,\n",
      "        -8.2930e-01,  9.3677e-01,  2.9041e+00,  2.2515e-01,  1.5148e+00,\n",
      "        -1.3885e+00, -7.6204e-01,  1.1389e+00, -8.6435e-01, -2.2556e-02,\n",
      "        -1.2408e+00,  1.9292e+00], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# src_emb : src -> 각각을 512 차원에 임베딩\n",
    "print(src_emb.weight)\n",
    "print(src_emb.weight.shape)\n",
    "print(src_vocab)\n",
    "print(src_emb.weight[0]) # P embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 1.2) pos_emb\n",
    "# self.pos_emb \n",
    "pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_vocab_size, d_model), freeze=True)\n",
    "\n",
    "# src_vocab_size(n_position) = 5, d_model = 512\n",
    "def get_sinusoid_encoding_table(n_position, d_model):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_vocab_size, d_model), freeze=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 512\n"
     ]
    }
   ],
   "source": [
    "# sinusoid_table 부터 만들어보자.\n",
    "\n",
    "def get_posi_angle_vec(position): # position 에는 0부터 4까지가 들어간다.\n",
    "    return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "def cal_angle(position, hid_idx): # hid_idx 로는 0부터 511까지가 들어간다. \n",
    "    return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "\n",
    "n_position = 5\n",
    "np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posi_angle_vec(position): # position 에는 0부터 4까지가 들어간다.\n",
    "    return [cal_angle(position, hid_j) for hid_j in range(d_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_angle(position, hid_idx): # hid_idx 로는 0부터 511까지가 들어간다. \n",
    "    return position / np.power(10000, 2 * (hid_idx // 2) / d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0,\n",
       " 4.0,\n",
       " 3.8586464796447966,\n",
       " 3.8586464796447966,\n",
       " 3.722288163718796,\n",
       " 3.722288163718796,\n",
       " 3.590748529789257,\n",
       " 3.590748529789257,\n",
       " 3.4638572934402614,\n",
       " 3.4638572934402614,\n",
       " 3.3414501878313048,\n",
       " 3.3414501878313048,\n",
       " 3.2233687510459275,\n",
       " 3.2233687510459275,\n",
       " 3.109460120955103,\n",
       " 3.109460120955103,\n",
       " 2.9995768373298235,\n",
       " 2.9995768373298235,\n",
       " 2.893576650946699,\n",
       " 2.893576650946699,\n",
       " 2.7913223394394655,\n",
       " 2.7913223394394655,\n",
       " 2.692681529657993,\n",
       " 2.692681529657993,\n",
       " 2.5975265263048453,\n",
       " 2.5975265263048453,\n",
       " 2.505734146627542,\n",
       " 2.505734146627542,\n",
       " 2.417185560952531,\n",
       " 2.417185560952531,\n",
       " 2.3317661388544297,\n",
       " 2.3317661388544297,\n",
       " 2.2493653007613963,\n",
       " 2.2493653007613963,\n",
       " 2.1698763748045304,\n",
       " 2.1698763748045304,\n",
       " 2.093196458725979,\n",
       " 2.093196458725979,\n",
       " 2.019226286666988,\n",
       " 2.019226286666988,\n",
       " 1.9478701006634522,\n",
       " 1.9478701006634522,\n",
       " 1.8790355266825967,\n",
       " 1.8790355266825967,\n",
       " 1.812633455040327,\n",
       " 1.812633455040327,\n",
       " 1.7485779250444358,\n",
       " 1.7485779250444358,\n",
       " 1.6867860137143291,\n",
       " 1.6867860137143291,\n",
       " 1.627177728433219,\n",
       " 1.627177728433219,\n",
       " 1.5696759033938144,\n",
       " 1.5696759033938144,\n",
       " 1.514206099703452,\n",
       " 1.514206099703452,\n",
       " 1.4606965090193507,\n",
       " 1.4606965090193507,\n",
       " 1.4090778605892404,\n",
       " 1.4090778605892404,\n",
       " 1.3592833315770236,\n",
       " 1.3592833315770236,\n",
       " 1.3112484605573833,\n",
       " 1.3112484605573833,\n",
       " 1.2649110640673518,\n",
       " 1.2649110640673518,\n",
       " 1.22021115610681,\n",
       " 1.22021115610681,\n",
       " 1.1770908704837126,\n",
       " 1.1770908704837126,\n",
       " 1.135494385903502,\n",
       " 1.135494385903502,\n",
       " 1.0953678537057445,\n",
       " 1.0953678537057445,\n",
       " 1.056659328154437,\n",
       " 1.056659328154437,\n",
       " 1.0193186991917387,\n",
       " 1.0193186991917387,\n",
       " 0.983297627568079,\n",
       " 0.983297627568079,\n",
       " 0.9485494822646622,\n",
       " 0.9485494822646622,\n",
       " 0.9150292801273583,\n",
       " 0.9150292801273583,\n",
       " 0.882693627633836,\n",
       " 0.882693627633836,\n",
       " 0.8515006647185491,\n",
       " 0.8515006647185491,\n",
       " 0.8214100105828585,\n",
       " 0.8214100105828585,\n",
       " 0.7923827114201355,\n",
       " 0.7923827114201355,\n",
       " 0.7643811899881762,\n",
       " 0.7643811899881762,\n",
       " 0.7373691969636441,\n",
       " 0.7373691969636441,\n",
       " 0.7113117640155691,\n",
       " 0.7113117640155691,\n",
       " 0.6861751585371516,\n",
       " 0.6861751585371516,\n",
       " 0.6619268399772725,\n",
       " 0.6619268399772725,\n",
       " 0.638535417715177,\n",
       " 0.638535417715177,\n",
       " 0.6159706104237967,\n",
       " 0.6159706104237967,\n",
       " 0.5942032068691101,\n",
       " 0.5942032068691101,\n",
       " 0.5732050280947851,\n",
       " 0.5732050280947851,\n",
       " 0.5529488909431598,\n",
       " 0.5529488909431598,\n",
       " 0.5334085728653296,\n",
       " 0.5334085728653296,\n",
       " 0.5145587779747898,\n",
       " 0.5145587779747898,\n",
       " 0.4963751043006879,\n",
       " 0.4963751043006879,\n",
       " 0.47883401219829197,\n",
       " 0.47883401219829197,\n",
       " 0.4619127938757832,\n",
       " 0.4619127938757832,\n",
       " 0.4455895439979209,\n",
       " 0.4455895439979209,\n",
       " 0.429843131328527,\n",
       " 0.429843131328527,\n",
       " 0.4146531713750792,\n",
       " 0.4146531713750792,\n",
       " 0.4,\n",
       " 0.4,\n",
       " 0.3858646479644797,\n",
       " 0.3858646479644797,\n",
       " 0.3722288163718796,\n",
       " 0.3722288163718796,\n",
       " 0.3590748529789257,\n",
       " 0.3590748529789257,\n",
       " 0.34638572934402617,\n",
       " 0.34638572934402617,\n",
       " 0.33414501878313047,\n",
       " 0.33414501878313047,\n",
       " 0.32233687510459275,\n",
       " 0.32233687510459275,\n",
       " 0.3109460120955103,\n",
       " 0.3109460120955103,\n",
       " 0.2999576837329823,\n",
       " 0.2999576837329823,\n",
       " 0.2893576650946699,\n",
       " 0.2893576650946699,\n",
       " 0.27913223394394654,\n",
       " 0.27913223394394654,\n",
       " 0.26926815296579926,\n",
       " 0.26926815296579926,\n",
       " 0.2597526526304845,\n",
       " 0.2597526526304845,\n",
       " 0.25057341466275423,\n",
       " 0.25057341466275423,\n",
       " 0.2417185560952531,\n",
       " 0.2417185560952531,\n",
       " 0.23317661388544295,\n",
       " 0.23317661388544295,\n",
       " 0.22493653007613962,\n",
       " 0.22493653007613962,\n",
       " 0.21698763748045302,\n",
       " 0.21698763748045302,\n",
       " 0.20931964587259788,\n",
       " 0.20931964587259788,\n",
       " 0.20192262866669886,\n",
       " 0.20192262866669886,\n",
       " 0.19478701006634525,\n",
       " 0.19478701006634525,\n",
       " 0.18790355266825967,\n",
       " 0.18790355266825967,\n",
       " 0.1812633455040327,\n",
       " 0.1812633455040327,\n",
       " 0.1748577925044436,\n",
       " 0.1748577925044436,\n",
       " 0.1686786013714329,\n",
       " 0.1686786013714329,\n",
       " 0.1627177728433219,\n",
       " 0.1627177728433219,\n",
       " 0.15696759033938143,\n",
       " 0.15696759033938143,\n",
       " 0.1514206099703452,\n",
       " 0.1514206099703452,\n",
       " 0.14606965090193508,\n",
       " 0.14606965090193508,\n",
       " 0.14090778605892407,\n",
       " 0.14090778605892407,\n",
       " 0.13592833315770236,\n",
       " 0.13592833315770236,\n",
       " 0.13112484605573835,\n",
       " 0.13112484605573835,\n",
       " 0.12649110640673517,\n",
       " 0.12649110640673517,\n",
       " 0.12202111561068102,\n",
       " 0.12202111561068102,\n",
       " 0.11770908704837127,\n",
       " 0.11770908704837127,\n",
       " 0.1135494385903502,\n",
       " 0.1135494385903502,\n",
       " 0.10953678537057444,\n",
       " 0.10953678537057444,\n",
       " 0.1056659328154437,\n",
       " 0.1056659328154437,\n",
       " 0.10193186991917386,\n",
       " 0.10193186991917386,\n",
       " 0.0983297627568079,\n",
       " 0.0983297627568079,\n",
       " 0.0948549482264662,\n",
       " 0.0948549482264662,\n",
       " 0.09150292801273584,\n",
       " 0.09150292801273584,\n",
       " 0.0882693627633836,\n",
       " 0.0882693627633836,\n",
       " 0.08515006647185491,\n",
       " 0.08515006647185491,\n",
       " 0.08214100105828585,\n",
       " 0.08214100105828585,\n",
       " 0.07923827114201355,\n",
       " 0.07923827114201355,\n",
       " 0.07643811899881763,\n",
       " 0.07643811899881763,\n",
       " 0.07373691969636442,\n",
       " 0.07373691969636442,\n",
       " 0.07113117640155692,\n",
       " 0.07113117640155692,\n",
       " 0.06861751585371516,\n",
       " 0.06861751585371516,\n",
       " 0.06619268399772726,\n",
       " 0.06619268399772726,\n",
       " 0.06385354177151768,\n",
       " 0.06385354177151768,\n",
       " 0.06159706104237968,\n",
       " 0.06159706104237968,\n",
       " 0.059420320686911006,\n",
       " 0.059420320686911006,\n",
       " 0.057320502809478514,\n",
       " 0.057320502809478514,\n",
       " 0.05529488909431598,\n",
       " 0.05529488909431598,\n",
       " 0.05334085728653296,\n",
       " 0.05334085728653296,\n",
       " 0.05145587779747898,\n",
       " 0.05145587779747898,\n",
       " 0.04963751043006879,\n",
       " 0.04963751043006879,\n",
       " 0.047883401219829196,\n",
       " 0.047883401219829196,\n",
       " 0.04619127938757833,\n",
       " 0.04619127938757833,\n",
       " 0.0445589543997921,\n",
       " 0.0445589543997921,\n",
       " 0.0429843131328527,\n",
       " 0.0429843131328527,\n",
       " 0.04146531713750792,\n",
       " 0.04146531713750792,\n",
       " 0.04,\n",
       " 0.04,\n",
       " 0.03858646479644797,\n",
       " 0.03858646479644797,\n",
       " 0.037222881637187955,\n",
       " 0.037222881637187955,\n",
       " 0.03590748529789257,\n",
       " 0.03590748529789257,\n",
       " 0.03463857293440261,\n",
       " 0.03463857293440261,\n",
       " 0.033414501878313047,\n",
       " 0.033414501878313047,\n",
       " 0.03223368751045927,\n",
       " 0.03223368751045927,\n",
       " 0.031094601209551032,\n",
       " 0.031094601209551032,\n",
       " 0.029995768373298235,\n",
       " 0.029995768373298235,\n",
       " 0.028935766509466988,\n",
       " 0.028935766509466988,\n",
       " 0.027913223394394653,\n",
       " 0.027913223394394653,\n",
       " 0.02692681529657993,\n",
       " 0.02692681529657993,\n",
       " 0.025975265263048455,\n",
       " 0.025975265263048455,\n",
       " 0.02505734146627542,\n",
       " 0.02505734146627542,\n",
       " 0.02417185560952531,\n",
       " 0.02417185560952531,\n",
       " 0.023317661388544297,\n",
       " 0.023317661388544297,\n",
       " 0.022493653007613964,\n",
       " 0.022493653007613964,\n",
       " 0.021698763748045306,\n",
       " 0.021698763748045306,\n",
       " 0.020931964587259787,\n",
       " 0.020931964587259787,\n",
       " 0.020192262866669884,\n",
       " 0.020192262866669884,\n",
       " 0.019478701006634527,\n",
       " 0.019478701006634527,\n",
       " 0.018790355266825966,\n",
       " 0.018790355266825966,\n",
       " 0.018126334550403272,\n",
       " 0.018126334550403272,\n",
       " 0.01748577925044436,\n",
       " 0.01748577925044436,\n",
       " 0.01686786013714329,\n",
       " 0.01686786013714329,\n",
       " 0.016271777284332187,\n",
       " 0.016271777284332187,\n",
       " 0.015696759033938145,\n",
       " 0.015696759033938145,\n",
       " 0.01514206099703452,\n",
       " 0.01514206099703452,\n",
       " 0.014606965090193508,\n",
       " 0.014606965090193508,\n",
       " 0.014090778605892404,\n",
       " 0.014090778605892404,\n",
       " 0.013592833315770238,\n",
       " 0.013592833315770238,\n",
       " 0.013112484605573835,\n",
       " 0.013112484605573835,\n",
       " 0.012649110640673516,\n",
       " 0.012649110640673516,\n",
       " 0.012202111561068103,\n",
       " 0.012202111561068103,\n",
       " 0.011770908704837126,\n",
       " 0.011770908704837126,\n",
       " 0.01135494385903502,\n",
       " 0.01135494385903502,\n",
       " 0.010953678537057445,\n",
       " 0.010953678537057445,\n",
       " 0.010566593281544369,\n",
       " 0.010566593281544369,\n",
       " 0.010193186991917385,\n",
       " 0.010193186991917385,\n",
       " 0.009832976275680791,\n",
       " 0.009832976275680791,\n",
       " 0.00948549482264662,\n",
       " 0.00948549482264662,\n",
       " 0.009150292801273583,\n",
       " 0.009150292801273583,\n",
       " 0.008826936276338359,\n",
       " 0.008826936276338359,\n",
       " 0.00851500664718549,\n",
       " 0.00851500664718549,\n",
       " 0.008214100105828584,\n",
       " 0.008214100105828584,\n",
       " 0.007923827114201355,\n",
       " 0.007923827114201355,\n",
       " 0.007643811899881762,\n",
       " 0.007643811899881762,\n",
       " 0.007373691969636442,\n",
       " 0.007373691969636442,\n",
       " 0.007113117640155692,\n",
       " 0.007113117640155692,\n",
       " 0.006861751585371515,\n",
       " 0.006861751585371515,\n",
       " 0.0066192683997727255,\n",
       " 0.0066192683997727255,\n",
       " 0.006385354177151769,\n",
       " 0.006385354177151769,\n",
       " 0.006159706104237967,\n",
       " 0.006159706104237967,\n",
       " 0.0059420320686911,\n",
       " 0.0059420320686911,\n",
       " 0.005732050280947851,\n",
       " 0.005732050280947851,\n",
       " 0.0055294889094315985,\n",
       " 0.0055294889094315985,\n",
       " 0.005334085728653296,\n",
       " 0.005334085728653296,\n",
       " 0.005145587779747898,\n",
       " 0.005145587779747898,\n",
       " 0.004963751043006878,\n",
       " 0.004963751043006878,\n",
       " 0.00478834012198292,\n",
       " 0.00478834012198292,\n",
       " 0.0046191279387578325,\n",
       " 0.0046191279387578325,\n",
       " 0.00445589543997921,\n",
       " 0.00445589543997921,\n",
       " 0.004298431313285269,\n",
       " 0.004298431313285269,\n",
       " 0.004146531713750792,\n",
       " 0.004146531713750792,\n",
       " 0.004,\n",
       " 0.004,\n",
       " 0.003858646479644797,\n",
       " 0.003858646479644797,\n",
       " 0.0037222881637187957,\n",
       " 0.0037222881637187957,\n",
       " 0.003590748529789257,\n",
       " 0.003590748529789257,\n",
       " 0.0034638572934402615,\n",
       " 0.0034638572934402615,\n",
       " 0.0033414501878313045,\n",
       " 0.0033414501878313045,\n",
       " 0.003223368751045927,\n",
       " 0.003223368751045927,\n",
       " 0.0031094601209551032,\n",
       " 0.0031094601209551032,\n",
       " 0.002999576837329823,\n",
       " 0.002999576837329823,\n",
       " 0.002893576650946699,\n",
       " 0.002893576650946699,\n",
       " 0.0027913223394394654,\n",
       " 0.0027913223394394654,\n",
       " 0.002692681529657993,\n",
       " 0.002692681529657993,\n",
       " 0.002597526526304845,\n",
       " 0.002597526526304845,\n",
       " 0.002505734146627542,\n",
       " 0.002505734146627542,\n",
       " 0.0024171855609525315,\n",
       " 0.0024171855609525315,\n",
       " 0.0023317661388544296,\n",
       " 0.0023317661388544296,\n",
       " 0.0022493653007613964,\n",
       " 0.0022493653007613964,\n",
       " 0.00216987637480453,\n",
       " 0.00216987637480453,\n",
       " 0.002093196458725979,\n",
       " 0.002093196458725979,\n",
       " 0.002019226286666988,\n",
       " 0.002019226286666988,\n",
       " 0.0019478701006634526,\n",
       " 0.0019478701006634526,\n",
       " 0.0018790355266825969,\n",
       " 0.0018790355266825969,\n",
       " 0.0018126334550403272,\n",
       " 0.0018126334550403272,\n",
       " 0.0017485779250444359,\n",
       " 0.0017485779250444359,\n",
       " 0.0016867860137143292,\n",
       " 0.0016867860137143292,\n",
       " 0.0016271777284332187,\n",
       " 0.0016271777284332187,\n",
       " 0.0015696759033938144,\n",
       " 0.0015696759033938144,\n",
       " 0.001514206099703452,\n",
       " 0.001514206099703452,\n",
       " 0.0014606965090193508,\n",
       " 0.0014606965090193508,\n",
       " 0.0014090778605892406,\n",
       " 0.0014090778605892406,\n",
       " 0.0013592833315770236,\n",
       " 0.0013592833315770236,\n",
       " 0.0013112484605573833,\n",
       " 0.0013112484605573833,\n",
       " 0.0012649110640673518,\n",
       " 0.0012649110640673518,\n",
       " 0.0012202111561068103,\n",
       " 0.0012202111561068103,\n",
       " 0.0011770908704837127,\n",
       " 0.0011770908704837127,\n",
       " 0.0011354943859035018,\n",
       " 0.0011354943859035018,\n",
       " 0.0010953678537057446,\n",
       " 0.0010953678537057446,\n",
       " 0.001056659328154437,\n",
       " 0.001056659328154437,\n",
       " 0.0010193186991917385,\n",
       " 0.0010193186991917385,\n",
       " 0.000983297627568079,\n",
       " 0.000983297627568079,\n",
       " 0.0009485494822646622,\n",
       " 0.0009485494822646622,\n",
       " 0.0009150292801273582,\n",
       " 0.0009150292801273582,\n",
       " 0.0008826936276338358,\n",
       " 0.0008826936276338358,\n",
       " 0.0008515006647185489,\n",
       " 0.0008515006647185489,\n",
       " 0.0008214100105828584,\n",
       " 0.0008214100105828584,\n",
       " 0.0007923827114201355,\n",
       " 0.0007923827114201355,\n",
       " 0.0007643811899881762,\n",
       " 0.0007643811899881762,\n",
       " 0.0007373691969636442,\n",
       " 0.0007373691969636442,\n",
       " 0.0007113117640155691,\n",
       " 0.0007113117640155691,\n",
       " 0.0006861751585371515,\n",
       " 0.0006861751585371515,\n",
       " 0.0006619268399772726,\n",
       " 0.0006619268399772726,\n",
       " 0.0006385354177151769,\n",
       " 0.0006385354177151769,\n",
       " 0.0006159706104237968,\n",
       " 0.0006159706104237968,\n",
       " 0.00059420320686911,\n",
       " 0.00059420320686911,\n",
       " 0.0005732050280947851,\n",
       " 0.0005732050280947851,\n",
       " 0.0005529488909431599,\n",
       " 0.0005529488909431599,\n",
       " 0.0005334085728653296,\n",
       " 0.0005334085728653296,\n",
       " 0.0005145587779747899,\n",
       " 0.0005145587779747899,\n",
       " 0.0004963751043006878,\n",
       " 0.0004963751043006878,\n",
       " 0.00047883401219829205,\n",
       " 0.00047883401219829205,\n",
       " 0.0004619127938757833,\n",
       " 0.0004619127938757833,\n",
       " 0.0004455895439979209,\n",
       " 0.0004455895439979209,\n",
       " 0.000429843131328527,\n",
       " 0.000429843131328527,\n",
       " 0.00041465317137507924,\n",
       " 0.00041465317137507924]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cal_angle(4, hid_j) for hid_j in range(d_model)] # 한번에 512개씩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 512)\n"
     ]
    }
   ],
   "source": [
    "sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "print(sinusoid_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00000000e+00, 2.00000000e+00, 1.92932324e+00, 1.92932324e+00,\n",
       "       1.86114408e+00, 1.86114408e+00, 1.79537426e+00, 1.79537426e+00,\n",
       "       1.73192865e+00, 1.73192865e+00, 1.67072509e+00, 1.67072509e+00,\n",
       "       1.61168438e+00, 1.61168438e+00, 1.55473006e+00, 1.55473006e+00,\n",
       "       1.49978842e+00, 1.49978842e+00, 1.44678833e+00, 1.44678833e+00,\n",
       "       1.39566117e+00, 1.39566117e+00, 1.34634076e+00, 1.34634076e+00,\n",
       "       1.29876326e+00, 1.29876326e+00, 1.25286707e+00, 1.25286707e+00,\n",
       "       1.20859278e+00, 1.20859278e+00, 1.16588307e+00, 1.16588307e+00,\n",
       "       1.12468265e+00, 1.12468265e+00, 1.08493819e+00, 1.08493819e+00,\n",
       "       1.04659823e+00, 1.04659823e+00, 1.00961314e+00, 1.00961314e+00,\n",
       "       9.73935050e-01, 9.73935050e-01, 9.39517763e-01, 9.39517763e-01,\n",
       "       9.06316728e-01, 9.06316728e-01, 8.74288963e-01, 8.74288963e-01,\n",
       "       8.43393007e-01, 8.43393007e-01, 8.13588864e-01, 8.13588864e-01,\n",
       "       7.84837952e-01, 7.84837952e-01, 7.57103050e-01, 7.57103050e-01,\n",
       "       7.30348255e-01, 7.30348255e-01, 7.04538930e-01, 7.04538930e-01,\n",
       "       6.79641666e-01, 6.79641666e-01, 6.55624230e-01, 6.55624230e-01,\n",
       "       6.32455532e-01, 6.32455532e-01, 6.10105578e-01, 6.10105578e-01,\n",
       "       5.88545435e-01, 5.88545435e-01, 5.67747193e-01, 5.67747193e-01,\n",
       "       5.47683927e-01, 5.47683927e-01, 5.28329664e-01, 5.28329664e-01,\n",
       "       5.09659350e-01, 5.09659350e-01, 4.91648814e-01, 4.91648814e-01,\n",
       "       4.74274741e-01, 4.74274741e-01, 4.57514640e-01, 4.57514640e-01,\n",
       "       4.41346814e-01, 4.41346814e-01, 4.25750332e-01, 4.25750332e-01,\n",
       "       4.10705005e-01, 4.10705005e-01, 3.96191356e-01, 3.96191356e-01,\n",
       "       3.82190595e-01, 3.82190595e-01, 3.68684598e-01, 3.68684598e-01,\n",
       "       3.55655882e-01, 3.55655882e-01, 3.43087579e-01, 3.43087579e-01,\n",
       "       3.30963420e-01, 3.30963420e-01, 3.19267709e-01, 3.19267709e-01,\n",
       "       3.07985305e-01, 3.07985305e-01, 2.97101603e-01, 2.97101603e-01,\n",
       "       2.86602514e-01, 2.86602514e-01, 2.76474445e-01, 2.76474445e-01,\n",
       "       2.66704286e-01, 2.66704286e-01, 2.57279389e-01, 2.57279389e-01,\n",
       "       2.48187552e-01, 2.48187552e-01, 2.39417006e-01, 2.39417006e-01,\n",
       "       2.30956397e-01, 2.30956397e-01, 2.22794772e-01, 2.22794772e-01,\n",
       "       2.14921566e-01, 2.14921566e-01, 2.07326586e-01, 2.07326586e-01,\n",
       "       2.00000000e-01, 2.00000000e-01, 1.92932324e-01, 1.92932324e-01,\n",
       "       1.86114408e-01, 1.86114408e-01, 1.79537426e-01, 1.79537426e-01,\n",
       "       1.73192865e-01, 1.73192865e-01, 1.67072509e-01, 1.67072509e-01,\n",
       "       1.61168438e-01, 1.61168438e-01, 1.55473006e-01, 1.55473006e-01,\n",
       "       1.49978842e-01, 1.49978842e-01, 1.44678833e-01, 1.44678833e-01,\n",
       "       1.39566117e-01, 1.39566117e-01, 1.34634076e-01, 1.34634076e-01,\n",
       "       1.29876326e-01, 1.29876326e-01, 1.25286707e-01, 1.25286707e-01,\n",
       "       1.20859278e-01, 1.20859278e-01, 1.16588307e-01, 1.16588307e-01,\n",
       "       1.12468265e-01, 1.12468265e-01, 1.08493819e-01, 1.08493819e-01,\n",
       "       1.04659823e-01, 1.04659823e-01, 1.00961314e-01, 1.00961314e-01,\n",
       "       9.73935050e-02, 9.73935050e-02, 9.39517763e-02, 9.39517763e-02,\n",
       "       9.06316728e-02, 9.06316728e-02, 8.74288963e-02, 8.74288963e-02,\n",
       "       8.43393007e-02, 8.43393007e-02, 8.13588864e-02, 8.13588864e-02,\n",
       "       7.84837952e-02, 7.84837952e-02, 7.57103050e-02, 7.57103050e-02,\n",
       "       7.30348255e-02, 7.30348255e-02, 7.04538930e-02, 7.04538930e-02,\n",
       "       6.79641666e-02, 6.79641666e-02, 6.55624230e-02, 6.55624230e-02,\n",
       "       6.32455532e-02, 6.32455532e-02, 6.10105578e-02, 6.10105578e-02,\n",
       "       5.88545435e-02, 5.88545435e-02, 5.67747193e-02, 5.67747193e-02,\n",
       "       5.47683927e-02, 5.47683927e-02, 5.28329664e-02, 5.28329664e-02,\n",
       "       5.09659350e-02, 5.09659350e-02, 4.91648814e-02, 4.91648814e-02,\n",
       "       4.74274741e-02, 4.74274741e-02, 4.57514640e-02, 4.57514640e-02,\n",
       "       4.41346814e-02, 4.41346814e-02, 4.25750332e-02, 4.25750332e-02,\n",
       "       4.10705005e-02, 4.10705005e-02, 3.96191356e-02, 3.96191356e-02,\n",
       "       3.82190595e-02, 3.82190595e-02, 3.68684598e-02, 3.68684598e-02,\n",
       "       3.55655882e-02, 3.55655882e-02, 3.43087579e-02, 3.43087579e-02,\n",
       "       3.30963420e-02, 3.30963420e-02, 3.19267709e-02, 3.19267709e-02,\n",
       "       3.07985305e-02, 3.07985305e-02, 2.97101603e-02, 2.97101603e-02,\n",
       "       2.86602514e-02, 2.86602514e-02, 2.76474445e-02, 2.76474445e-02,\n",
       "       2.66704286e-02, 2.66704286e-02, 2.57279389e-02, 2.57279389e-02,\n",
       "       2.48187552e-02, 2.48187552e-02, 2.39417006e-02, 2.39417006e-02,\n",
       "       2.30956397e-02, 2.30956397e-02, 2.22794772e-02, 2.22794772e-02,\n",
       "       2.14921566e-02, 2.14921566e-02, 2.07326586e-02, 2.07326586e-02,\n",
       "       2.00000000e-02, 2.00000000e-02, 1.92932324e-02, 1.92932324e-02,\n",
       "       1.86114408e-02, 1.86114408e-02, 1.79537426e-02, 1.79537426e-02,\n",
       "       1.73192865e-02, 1.73192865e-02, 1.67072509e-02, 1.67072509e-02,\n",
       "       1.61168438e-02, 1.61168438e-02, 1.55473006e-02, 1.55473006e-02,\n",
       "       1.49978842e-02, 1.49978842e-02, 1.44678833e-02, 1.44678833e-02,\n",
       "       1.39566117e-02, 1.39566117e-02, 1.34634076e-02, 1.34634076e-02,\n",
       "       1.29876326e-02, 1.29876326e-02, 1.25286707e-02, 1.25286707e-02,\n",
       "       1.20859278e-02, 1.20859278e-02, 1.16588307e-02, 1.16588307e-02,\n",
       "       1.12468265e-02, 1.12468265e-02, 1.08493819e-02, 1.08493819e-02,\n",
       "       1.04659823e-02, 1.04659823e-02, 1.00961314e-02, 1.00961314e-02,\n",
       "       9.73935050e-03, 9.73935050e-03, 9.39517763e-03, 9.39517763e-03,\n",
       "       9.06316728e-03, 9.06316728e-03, 8.74288963e-03, 8.74288963e-03,\n",
       "       8.43393007e-03, 8.43393007e-03, 8.13588864e-03, 8.13588864e-03,\n",
       "       7.84837952e-03, 7.84837952e-03, 7.57103050e-03, 7.57103050e-03,\n",
       "       7.30348255e-03, 7.30348255e-03, 7.04538930e-03, 7.04538930e-03,\n",
       "       6.79641666e-03, 6.79641666e-03, 6.55624230e-03, 6.55624230e-03,\n",
       "       6.32455532e-03, 6.32455532e-03, 6.10105578e-03, 6.10105578e-03,\n",
       "       5.88545435e-03, 5.88545435e-03, 5.67747193e-03, 5.67747193e-03,\n",
       "       5.47683927e-03, 5.47683927e-03, 5.28329664e-03, 5.28329664e-03,\n",
       "       5.09659350e-03, 5.09659350e-03, 4.91648814e-03, 4.91648814e-03,\n",
       "       4.74274741e-03, 4.74274741e-03, 4.57514640e-03, 4.57514640e-03,\n",
       "       4.41346814e-03, 4.41346814e-03, 4.25750332e-03, 4.25750332e-03,\n",
       "       4.10705005e-03, 4.10705005e-03, 3.96191356e-03, 3.96191356e-03,\n",
       "       3.82190595e-03, 3.82190595e-03, 3.68684598e-03, 3.68684598e-03,\n",
       "       3.55655882e-03, 3.55655882e-03, 3.43087579e-03, 3.43087579e-03,\n",
       "       3.30963420e-03, 3.30963420e-03, 3.19267709e-03, 3.19267709e-03,\n",
       "       3.07985305e-03, 3.07985305e-03, 2.97101603e-03, 2.97101603e-03,\n",
       "       2.86602514e-03, 2.86602514e-03, 2.76474445e-03, 2.76474445e-03,\n",
       "       2.66704286e-03, 2.66704286e-03, 2.57279389e-03, 2.57279389e-03,\n",
       "       2.48187552e-03, 2.48187552e-03, 2.39417006e-03, 2.39417006e-03,\n",
       "       2.30956397e-03, 2.30956397e-03, 2.22794772e-03, 2.22794772e-03,\n",
       "       2.14921566e-03, 2.14921566e-03, 2.07326586e-03, 2.07326586e-03,\n",
       "       2.00000000e-03, 2.00000000e-03, 1.92932324e-03, 1.92932324e-03,\n",
       "       1.86114408e-03, 1.86114408e-03, 1.79537426e-03, 1.79537426e-03,\n",
       "       1.73192865e-03, 1.73192865e-03, 1.67072509e-03, 1.67072509e-03,\n",
       "       1.61168438e-03, 1.61168438e-03, 1.55473006e-03, 1.55473006e-03,\n",
       "       1.49978842e-03, 1.49978842e-03, 1.44678833e-03, 1.44678833e-03,\n",
       "       1.39566117e-03, 1.39566117e-03, 1.34634076e-03, 1.34634076e-03,\n",
       "       1.29876326e-03, 1.29876326e-03, 1.25286707e-03, 1.25286707e-03,\n",
       "       1.20859278e-03, 1.20859278e-03, 1.16588307e-03, 1.16588307e-03,\n",
       "       1.12468265e-03, 1.12468265e-03, 1.08493819e-03, 1.08493819e-03,\n",
       "       1.04659823e-03, 1.04659823e-03, 1.00961314e-03, 1.00961314e-03,\n",
       "       9.73935050e-04, 9.73935050e-04, 9.39517763e-04, 9.39517763e-04,\n",
       "       9.06316728e-04, 9.06316728e-04, 8.74288963e-04, 8.74288963e-04,\n",
       "       8.43393007e-04, 8.43393007e-04, 8.13588864e-04, 8.13588864e-04,\n",
       "       7.84837952e-04, 7.84837952e-04, 7.57103050e-04, 7.57103050e-04,\n",
       "       7.30348255e-04, 7.30348255e-04, 7.04538930e-04, 7.04538930e-04,\n",
       "       6.79641666e-04, 6.79641666e-04, 6.55624230e-04, 6.55624230e-04,\n",
       "       6.32455532e-04, 6.32455532e-04, 6.10105578e-04, 6.10105578e-04,\n",
       "       5.88545435e-04, 5.88545435e-04, 5.67747193e-04, 5.67747193e-04,\n",
       "       5.47683927e-04, 5.47683927e-04, 5.28329664e-04, 5.28329664e-04,\n",
       "       5.09659350e-04, 5.09659350e-04, 4.91648814e-04, 4.91648814e-04,\n",
       "       4.74274741e-04, 4.74274741e-04, 4.57514640e-04, 4.57514640e-04,\n",
       "       4.41346814e-04, 4.41346814e-04, 4.25750332e-04, 4.25750332e-04,\n",
       "       4.10705005e-04, 4.10705005e-04, 3.96191356e-04, 3.96191356e-04,\n",
       "       3.82190595e-04, 3.82190595e-04, 3.68684598e-04, 3.68684598e-04,\n",
       "       3.55655882e-04, 3.55655882e-04, 3.43087579e-04, 3.43087579e-04,\n",
       "       3.30963420e-04, 3.30963420e-04, 3.19267709e-04, 3.19267709e-04,\n",
       "       3.07985305e-04, 3.07985305e-04, 2.97101603e-04, 2.97101603e-04,\n",
       "       2.86602514e-04, 2.86602514e-04, 2.76474445e-04, 2.76474445e-04,\n",
       "       2.66704286e-04, 2.66704286e-04, 2.57279389e-04, 2.57279389e-04,\n",
       "       2.48187552e-04, 2.48187552e-04, 2.39417006e-04, 2.39417006e-04,\n",
       "       2.30956397e-04, 2.30956397e-04, 2.22794772e-04, 2.22794772e-04,\n",
       "       2.14921566e-04, 2.14921566e-04, 2.07326586e-04, 2.07326586e-04])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoid_table[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":: 의미\n",
    "```python\n",
    "a = [1,2,3,4,5,6,7,8,9]\n",
    "a[2::3]\n",
    "\n",
    "# [3, 6, 9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 256)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoid_table[:, 0::2].shape # 0, 2, 4, ..., 510 (511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00000000e+00, 1.92932324e+00, 1.86114408e+00, 1.79537426e+00,\n",
       "       1.73192865e+00, 1.67072509e+00, 1.61168438e+00, 1.55473006e+00,\n",
       "       1.49978842e+00, 1.44678833e+00, 1.39566117e+00, 1.34634076e+00,\n",
       "       1.29876326e+00, 1.25286707e+00, 1.20859278e+00, 1.16588307e+00,\n",
       "       1.12468265e+00, 1.08493819e+00, 1.04659823e+00, 1.00961314e+00,\n",
       "       9.73935050e-01, 9.39517763e-01, 9.06316728e-01, 8.74288963e-01,\n",
       "       8.43393007e-01, 8.13588864e-01, 7.84837952e-01, 7.57103050e-01,\n",
       "       7.30348255e-01, 7.04538930e-01, 6.79641666e-01, 6.55624230e-01,\n",
       "       6.32455532e-01, 6.10105578e-01, 5.88545435e-01, 5.67747193e-01,\n",
       "       5.47683927e-01, 5.28329664e-01, 5.09659350e-01, 4.91648814e-01,\n",
       "       4.74274741e-01, 4.57514640e-01, 4.41346814e-01, 4.25750332e-01,\n",
       "       4.10705005e-01, 3.96191356e-01, 3.82190595e-01, 3.68684598e-01,\n",
       "       3.55655882e-01, 3.43087579e-01, 3.30963420e-01, 3.19267709e-01,\n",
       "       3.07985305e-01, 2.97101603e-01, 2.86602514e-01, 2.76474445e-01,\n",
       "       2.66704286e-01, 2.57279389e-01, 2.48187552e-01, 2.39417006e-01,\n",
       "       2.30956397e-01, 2.22794772e-01, 2.14921566e-01, 2.07326586e-01,\n",
       "       2.00000000e-01, 1.92932324e-01, 1.86114408e-01, 1.79537426e-01,\n",
       "       1.73192865e-01, 1.67072509e-01, 1.61168438e-01, 1.55473006e-01,\n",
       "       1.49978842e-01, 1.44678833e-01, 1.39566117e-01, 1.34634076e-01,\n",
       "       1.29876326e-01, 1.25286707e-01, 1.20859278e-01, 1.16588307e-01,\n",
       "       1.12468265e-01, 1.08493819e-01, 1.04659823e-01, 1.00961314e-01,\n",
       "       9.73935050e-02, 9.39517763e-02, 9.06316728e-02, 8.74288963e-02,\n",
       "       8.43393007e-02, 8.13588864e-02, 7.84837952e-02, 7.57103050e-02,\n",
       "       7.30348255e-02, 7.04538930e-02, 6.79641666e-02, 6.55624230e-02,\n",
       "       6.32455532e-02, 6.10105578e-02, 5.88545435e-02, 5.67747193e-02,\n",
       "       5.47683927e-02, 5.28329664e-02, 5.09659350e-02, 4.91648814e-02,\n",
       "       4.74274741e-02, 4.57514640e-02, 4.41346814e-02, 4.25750332e-02,\n",
       "       4.10705005e-02, 3.96191356e-02, 3.82190595e-02, 3.68684598e-02,\n",
       "       3.55655882e-02, 3.43087579e-02, 3.30963420e-02, 3.19267709e-02,\n",
       "       3.07985305e-02, 2.97101603e-02, 2.86602514e-02, 2.76474445e-02,\n",
       "       2.66704286e-02, 2.57279389e-02, 2.48187552e-02, 2.39417006e-02,\n",
       "       2.30956397e-02, 2.22794772e-02, 2.14921566e-02, 2.07326586e-02,\n",
       "       2.00000000e-02, 1.92932324e-02, 1.86114408e-02, 1.79537426e-02,\n",
       "       1.73192865e-02, 1.67072509e-02, 1.61168438e-02, 1.55473006e-02,\n",
       "       1.49978842e-02, 1.44678833e-02, 1.39566117e-02, 1.34634076e-02,\n",
       "       1.29876326e-02, 1.25286707e-02, 1.20859278e-02, 1.16588307e-02,\n",
       "       1.12468265e-02, 1.08493819e-02, 1.04659823e-02, 1.00961314e-02,\n",
       "       9.73935050e-03, 9.39517763e-03, 9.06316728e-03, 8.74288963e-03,\n",
       "       8.43393007e-03, 8.13588864e-03, 7.84837952e-03, 7.57103050e-03,\n",
       "       7.30348255e-03, 7.04538930e-03, 6.79641666e-03, 6.55624230e-03,\n",
       "       6.32455532e-03, 6.10105578e-03, 5.88545435e-03, 5.67747193e-03,\n",
       "       5.47683927e-03, 5.28329664e-03, 5.09659350e-03, 4.91648814e-03,\n",
       "       4.74274741e-03, 4.57514640e-03, 4.41346814e-03, 4.25750332e-03,\n",
       "       4.10705005e-03, 3.96191356e-03, 3.82190595e-03, 3.68684598e-03,\n",
       "       3.55655882e-03, 3.43087579e-03, 3.30963420e-03, 3.19267709e-03,\n",
       "       3.07985305e-03, 2.97101603e-03, 2.86602514e-03, 2.76474445e-03,\n",
       "       2.66704286e-03, 2.57279389e-03, 2.48187552e-03, 2.39417006e-03,\n",
       "       2.30956397e-03, 2.22794772e-03, 2.14921566e-03, 2.07326586e-03,\n",
       "       2.00000000e-03, 1.92932324e-03, 1.86114408e-03, 1.79537426e-03,\n",
       "       1.73192865e-03, 1.67072509e-03, 1.61168438e-03, 1.55473006e-03,\n",
       "       1.49978842e-03, 1.44678833e-03, 1.39566117e-03, 1.34634076e-03,\n",
       "       1.29876326e-03, 1.25286707e-03, 1.20859278e-03, 1.16588307e-03,\n",
       "       1.12468265e-03, 1.08493819e-03, 1.04659823e-03, 1.00961314e-03,\n",
       "       9.73935050e-04, 9.39517763e-04, 9.06316728e-04, 8.74288963e-04,\n",
       "       8.43393007e-04, 8.13588864e-04, 7.84837952e-04, 7.57103050e-04,\n",
       "       7.30348255e-04, 7.04538930e-04, 6.79641666e-04, 6.55624230e-04,\n",
       "       6.32455532e-04, 6.10105578e-04, 5.88545435e-04, 5.67747193e-04,\n",
       "       5.47683927e-04, 5.28329664e-04, 5.09659350e-04, 4.91648814e-04,\n",
       "       4.74274741e-04, 4.57514640e-04, 4.41346814e-04, 4.25750332e-04,\n",
       "       4.10705005e-04, 3.96191356e-04, 3.82190595e-04, 3.68684598e-04,\n",
       "       3.55655882e-04, 3.43087579e-04, 3.30963420e-04, 3.19267709e-04,\n",
       "       3.07985305e-04, 2.97101603e-04, 2.86602514e-04, 2.76474445e-04,\n",
       "       2.66704286e-04, 2.57279389e-04, 2.48187552e-04, 2.39417006e-04,\n",
       "       2.30956397e-04, 2.22794772e-04, 2.14921566e-04, 2.07326586e-04])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoid_table[:, 0::2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.09297427e-01, 9.36414739e-01, 9.58144376e-01, 9.74888185e-01,\n",
       "       9.87046251e-01, 9.95011274e-01, 9.99164200e-01, 9.99870940e-01,\n",
       "       9.97479998e-01, 9.92320856e-01, 9.84702998e-01, 9.74915430e-01,\n",
       "       9.63226623e-01, 9.49884770e-01, 9.35118300e-01, 9.19136572e-01,\n",
       "       9.02130715e-01, 8.84274552e-01, 8.65725587e-01, 8.46626027e-01,\n",
       "       8.27103803e-01, 8.07273589e-01, 7.87237797e-01, 7.67087535e-01,\n",
       "       7.46903535e-01, 7.26757021e-01, 7.06710541e-01, 6.86818743e-01,\n",
       "       6.67129105e-01, 6.47682605e-01, 6.28514353e-01, 6.09654169e-01,\n",
       "       5.91127117e-01, 5.72953994e-01, 5.55151778e-01, 5.37734041e-01,\n",
       "       5.20711320e-01, 5.04091459e-01, 4.87879918e-01, 4.72080050e-01,\n",
       "       4.56693360e-01, 4.41719725e-01, 4.27157610e-01, 4.13004247e-01,\n",
       "       3.99255804e-01, 3.85907533e-01, 3.72953906e-01, 3.60388735e-01,\n",
       "       3.48205276e-01, 3.36396328e-01, 3.24954314e-01, 3.13871360e-01,\n",
       "       3.03139357e-01, 2.92750025e-01, 2.82694961e-01, 2.72965685e-01,\n",
       "       2.63553681e-01, 2.54450432e-01, 2.45647451e-01, 2.37136302e-01,\n",
       "       2.28908631e-01, 2.20956178e-01, 2.13270799e-01, 2.05844476e-01,\n",
       "       1.98669331e-01, 1.91737633e-01, 1.85041811e-01, 1.78574454e-01,\n",
       "       1.72328320e-01, 1.66296338e-01, 1.60471611e-01, 1.54847417e-01,\n",
       "       1.49417212e-01, 1.44174625e-01, 1.39113464e-01, 1.34227708e-01,\n",
       "       1.29511512e-01, 1.24959199e-01, 1.20565262e-01, 1.16324359e-01,\n",
       "       1.12231311e-01, 1.08281099e-01, 1.04468859e-01, 1.00789882e-01,\n",
       "       9.72396071e-02, 9.38136196e-02, 9.05076474e-02, 8.73175571e-02,\n",
       "       8.42393503e-02, 8.12691601e-02, 7.84032471e-02, 7.56379965e-02,\n",
       "       7.29699138e-02, 7.03956216e-02, 6.79118561e-02, 6.55154639e-02,\n",
       "       6.32033979e-02, 6.09727150e-02, 5.88205721e-02, 5.67442232e-02,\n",
       "       5.47410165e-02, 5.28083909e-02, 5.09438736e-02, 4.91450770e-02,\n",
       "       4.74096958e-02, 4.57355045e-02, 4.41203547e-02, 4.25621722e-02,\n",
       "       4.10589553e-02, 3.96087715e-02, 3.82097558e-02, 3.68601080e-02,\n",
       "       3.55580908e-02, 3.43020276e-02, 3.30903002e-02, 3.19213472e-02,\n",
       "       3.07936618e-02, 2.97057897e-02, 2.86563279e-02, 2.76439225e-02,\n",
       "       2.66672669e-02, 2.57251007e-02, 2.48162074e-02, 2.39394134e-02,\n",
       "       2.30935865e-02, 2.22776341e-02, 2.14905020e-02, 2.07311733e-02,\n",
       "       1.99986667e-02, 1.92920355e-02, 1.86103664e-02, 1.79527781e-02,\n",
       "       1.73184206e-02, 1.67064737e-02, 1.61161460e-02, 1.55466743e-02,\n",
       "       1.49973219e-02, 1.44673785e-02, 1.39561586e-02, 1.34630009e-02,\n",
       "       1.29872675e-02, 1.25283430e-02, 1.20856336e-02, 1.16585666e-02,\n",
       "       1.12465894e-02, 1.08491690e-02, 1.04657912e-02, 1.00959599e-02,\n",
       "       9.73919653e-03, 9.39503942e-03, 9.06304320e-03, 8.74277824e-03,\n",
       "       8.43383008e-03, 8.13579889e-03, 7.84829894e-03, 7.57095817e-03,\n",
       "       7.30341762e-03, 7.04533102e-03, 6.79636434e-03, 6.55619533e-03,\n",
       "       6.32451316e-03, 6.10101793e-03, 5.88542038e-03, 5.67744143e-03,\n",
       "       5.47681189e-03, 5.28327206e-03, 5.09657143e-03, 4.91646833e-03,\n",
       "       4.74272963e-03, 4.57513044e-03, 4.41345381e-03, 4.25749046e-03,\n",
       "       4.10703851e-03, 3.96190319e-03, 3.82189665e-03, 3.68683763e-03,\n",
       "       3.55655132e-03, 3.43086906e-03, 3.30962816e-03, 3.19267166e-03,\n",
       "       3.07984818e-03, 2.97101166e-03, 2.86602122e-03, 2.76474093e-03,\n",
       "       2.66703970e-03, 2.57279105e-03, 2.48187297e-03, 2.39416777e-03,\n",
       "       2.30956192e-03, 2.22794588e-03, 2.14921400e-03, 2.07326437e-03,\n",
       "       1.99999867e-03, 1.92932204e-03, 1.86114301e-03, 1.79537330e-03,\n",
       "       1.73192778e-03, 1.67072432e-03, 1.61168368e-03, 1.55472943e-03,\n",
       "       1.49978786e-03, 1.44678782e-03, 1.39566072e-03, 1.34634036e-03,\n",
       "       1.29876290e-03, 1.25286675e-03, 1.20859249e-03, 1.16588281e-03,\n",
       "       1.12468241e-03, 1.08493797e-03, 1.04659804e-03, 1.00961297e-03,\n",
       "       9.73934896e-04, 9.39517625e-04, 9.06316603e-04, 8.74288851e-04,\n",
       "       8.43392907e-04, 8.13588774e-04, 7.84837871e-04, 7.57102978e-04,\n",
       "       7.30348190e-04, 7.04538872e-04, 6.79641613e-04, 6.55624183e-04,\n",
       "       6.32455490e-04, 6.10105540e-04, 5.88545401e-04, 5.67747162e-04,\n",
       "       5.47683899e-04, 5.28329639e-04, 5.09659328e-04, 4.91648794e-04,\n",
       "       4.74274723e-04, 4.57514624e-04, 4.41346799e-04, 4.25750319e-04,\n",
       "       4.10704994e-04, 3.96191345e-04, 3.82190586e-04, 3.68684590e-04,\n",
       "       3.55655875e-04, 3.43087573e-04, 3.30963414e-04, 3.19267703e-04,\n",
       "       3.07985300e-04, 2.97101599e-04, 2.86602510e-04, 2.76474442e-04,\n",
       "       2.66704283e-04, 2.57279386e-04, 2.48187550e-04, 2.39417004e-04,\n",
       "       2.30956395e-04, 2.22794770e-04, 2.14921564e-04, 2.07326584e-04])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoid_table[:, 0::2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.41614684, -0.35089519, -0.28628544, -0.22269492, -0.16043596,\n",
       "       -0.09976254, -0.04087666,  0.01606558,  0.07094825,  0.12369041,\n",
       "        0.17424123,  0.22257561,  0.26869029,  0.31260026,  0.35433567,\n",
       "        0.39393903,  0.43146283,  0.46696736,  0.50051894,  0.53218828,\n",
       "        0.5620492 ,  0.59017739,  0.61664954,  0.64154245,  0.66493241,\n",
       "        0.68689463,  0.7075028 ,  0.72682874,  0.74494212,  0.76191026,\n",
       "        0.77779799,  0.79266752,  0.80657841,  0.81958753,  0.83174906,\n",
       "        0.84311452,  0.85373282,  0.86365028,  0.87291075,  0.88155569,\n",
       "        0.88962418,  0.8971531 ,  0.90417718,  0.9107291 ,  0.91683957,\n",
       "        0.92253747,  0.92784987,  0.93280221,  0.93741831,  0.94172051,\n",
       "        0.94572971,  0.94946552,  0.95294624,  0.95618901,  0.95920986,\n",
       "        0.96202377,  0.96464473,  0.96708582,  0.96935924,  0.97147639,\n",
       "        0.97344791,  0.97528374,  0.97699312,  0.97858472,  0.98006658,\n",
       "        0.98144622,  0.98273065,  0.9839264 ,  0.98503957,  0.98607582,\n",
       "        0.98704046,  0.9879384 ,  0.98877424,  0.98955226,  0.99027645,\n",
       "        0.99095051,  0.99157792,  0.99216188,  0.9927054 ,  0.99321128,\n",
       "        0.99368211,  0.99412032,  0.99452816,  0.99490773,  0.995261  ,\n",
       "        0.99558978,  0.99589576,  0.99618053,  0.99644555,  0.99669219,\n",
       "        0.99692173,  0.99713534,  0.99733414,  0.99751915,  0.99769132,\n",
       "        0.99785155,  0.99800067,  0.99813943,  0.99826857,  0.99838875,\n",
       "        0.99850059,  0.99860466,  0.99870152,  0.99879165,  0.99887553,\n",
       "        0.99895358,  0.99902622,  0.99909382,  0.99915673,  0.99921526,\n",
       "        0.99926974,  0.99932044,  0.99936761,  0.99941151,  0.99945237,\n",
       "        0.99949038,  0.99952576,  0.99955869,  0.99958932,  0.99961783,\n",
       "        0.99964437,  0.99966905,  0.99969203,  0.99971341,  0.99973331,\n",
       "        0.99975182,  0.99976905,  0.99978509,  0.99980001,  0.99981389,\n",
       "        0.99982681,  0.99983884,  0.99985002,  0.99986044,  0.99987013,\n",
       "        0.99987914,  0.99988753,  0.99989534,  0.99990261,  0.99990937,\n",
       "        0.99991566,  0.99992152,  0.99992697,  0.99993204,  0.99993676,\n",
       "        0.99994115,  0.99994523,  0.99994903,  0.99995257,  0.99995587,\n",
       "        0.99995893,  0.99996178,  0.99996443,  0.9999669 ,  0.9999692 ,\n",
       "        0.99997134,  0.99997333,  0.99997518,  0.9999769 ,  0.99997851,\n",
       "        0.99998   ,  0.99998139,  0.99998268,  0.99998388,  0.999985  ,\n",
       "        0.99998604,  0.99998701,  0.99998791,  0.99998875,  0.99998953,\n",
       "        0.99999026,  0.99999094,  0.99999157,  0.99999215,  0.9999927 ,\n",
       "        0.9999932 ,  0.99999368,  0.99999411,  0.99999452,  0.9999949 ,\n",
       "        0.99999526,  0.99999559,  0.99999589,  0.99999618,  0.99999644,\n",
       "        0.99999669,  0.99999692,  0.99999713,  0.99999733,  0.99999752,\n",
       "        0.99999769,  0.99999785,  0.999998  ,  0.99999814,  0.99999827,\n",
       "        0.99999839,  0.9999985 ,  0.9999986 ,  0.9999987 ,  0.99999879,\n",
       "        0.99999888,  0.99999895,  0.99999903,  0.99999909,  0.99999916,\n",
       "        0.99999922,  0.99999927,  0.99999932,  0.99999937,  0.99999941,\n",
       "        0.99999945,  0.99999949,  0.99999953,  0.99999956,  0.99999959,\n",
       "        0.99999962,  0.99999964,  0.99999967,  0.99999969,  0.99999971,\n",
       "        0.99999973,  0.99999975,  0.99999977,  0.99999979,  0.9999998 ,\n",
       "        0.99999981,  0.99999983,  0.99999984,  0.99999985,  0.99999986,\n",
       "        0.99999987,  0.99999988,  0.99999989,  0.9999999 ,  0.9999999 ,\n",
       "        0.99999991,  0.99999992,  0.99999992,  0.99999993,  0.99999993,\n",
       "        0.99999994,  0.99999994,  0.99999995,  0.99999995,  0.99999995,\n",
       "        0.99999996,  0.99999996,  0.99999996,  0.99999996,  0.99999997,\n",
       "        0.99999997,  0.99999997,  0.99999997,  0.99999998,  0.99999998,\n",
       "        0.99999998])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinusoid_table[:, 1::2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_vocab_size, d_model), freeze=True)\n",
    "pos_emb = nn.Embedding.from_pretrained(torch.FloatTensor(sinusoid_table), freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3) enc_outputs\n",
    "enc_outputs = src_emb(enc_inputs) + pos_emb(enc_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb(enc_inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb(enc_inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8975,  1.5275,  0.5599,  ...,  0.0860,  0.7317,  0.8330],\n",
      "         [ 1.0840, -1.5479,  0.4180,  ...,  0.2803, -0.0269,  0.5393],\n",
      "         [-0.7122,  0.5253, -1.0219,  ...,  0.9817,  0.2214,  1.0245],\n",
      "         [-2.5731,  0.6607, -3.0326,  ..., -0.1859,  0.4281,  0.3123],\n",
      "         [-0.1280,  3.3881,  0.0602,  ...,  0.9774, -1.2408,  2.9292]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "print(enc_outputs)\n",
    "print(enc_outputs.shape)\n",
    "\n",
    "## enc_outputs 의미 : enc_inputs의 src_emb + pos_emb \n",
    "## -> 1 batch, 5 tokens, 512 embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 1.4) enc_self_attn_mask\n",
    "enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 0]])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(enc_inputs)\n",
    "print(enc_inputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5 5\n"
     ]
    }
   ],
   "source": [
    "batch_size, len_q = enc_inputs.size()\n",
    "batch_size, len_k = enc_inputs.size()\n",
    "print(batch_size, len_q, len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 0]])\n",
      "tensor([[0, 0, 0, 0, 1]], dtype=torch.uint8)\n",
      "tensor([[[0, 0, 0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(enc_inputs.data)\n",
    "print(enc_inputs.data.eq(0))\n",
    "print(enc_inputs.data.eq(0).unsqueeze(1))\n",
    "print(enc_inputs.data.eq(0).unsqueeze(1).shape) # index=1 크기 하나 늘리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "pad_attn_mask = enc_inputs.data.eq(0).unsqueeze(1)\n",
    "print(pad_attn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 1]]], dtype=torch.uint8)\n",
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "enc_self_attn_mask = pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "print(pad_attn_mask.expand(batch_size, len_q, len_k))\n",
    "print(pad_attn_mask.expand(batch_size, len_q, len_k).shape)\n",
    "## enc_self_attn_mask 이렇게 하는 이유 : 여기는 패딩이라 안볼거야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# 1.5) enc_self_attns\n",
    "layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "enc_self_attns = []\n",
    "for layer in self.layers:\n",
    "    enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "    enc_self_attns.append(enc_self_attn)\n",
    "\n",
    "# EncoderLayer() 가 필요!\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "```\n",
    "MultiHeadAttention() ---- Q: enc_outputs, K: enc_outputs, V: enc_outputs, attn_mask: enc_self_attn_mask (enc_inputs 아님 주의)  \n",
    "PoswiseFeedForwardNet() ---- inputs: enc_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8975,  1.5275,  0.5599,  ...,  0.0860,  0.7317,  0.8330],\n",
      "         [ 1.0840, -1.5479,  0.4180,  ...,  0.2803, -0.0269,  0.5393],\n",
      "         [-0.7122,  0.5253, -1.0219,  ...,  0.9817,  0.2214,  1.0245],\n",
      "         [-2.5731,  0.6607, -3.0326,  ..., -0.1859,  0.4281,  0.3123],\n",
      "         [-0.1280,  3.3881,  0.0602,  ...,  0.9774, -1.2408,  2.9292]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "print(enc_outputs)\n",
    "print(enc_outputs.shape) # 1 batch, 5 sentences, 512 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_self_attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# MultiHeadAttention(), PoswiseFeedForwardNet() 필요!!\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=512, bias=True)\n"
     ]
    }
   ],
   "source": [
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_heads = 8  # number of heads in Multi-Head Attention\n",
    "\n",
    "# key, query, value 에 대한 각각의 weight matrix\n",
    "W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "print(W_Q)\n",
    "print(W_K)\n",
    "print(W_V)\n",
    "# 512 -> 512 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_outputs 의미 : 'ich mochte ein bier P' (5 tokens -> 512 )\n",
    "Q, K, V, attn_mask = enc_outputs, enc_outputs, enc_outputs, enc_self_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8975,  1.5275,  0.5599,  ...,  0.0860,  0.7317,  0.8330],\n",
      "         [ 1.0840, -1.5479,  0.4180,  ...,  0.2803, -0.0269,  0.5393],\n",
      "         [-0.7122,  0.5253, -1.0219,  ...,  0.9817,  0.2214,  1.0245],\n",
      "         [-2.5731,  0.6607, -3.0326,  ..., -0.1859,  0.4281,  0.3123],\n",
      "         [-0.1280,  3.3881,  0.0602,  ...,  0.9774, -1.2408,  2.9292]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(Q)\n",
    "print(Q.size())\n",
    "print(Q.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8975,  1.5275,  0.5599,  ...,  0.0860,  0.7317,  0.8330],\n",
      "         [ 1.0840, -1.5479,  0.4180,  ...,  0.2803, -0.0269,  0.5393],\n",
      "         [-0.7122,  0.5253, -1.0219,  ...,  0.9817,  0.2214,  1.0245],\n",
      "         [-2.5731,  0.6607, -3.0326,  ..., -0.1859,  0.4281,  0.3123],\n",
      "         [-0.1280,  3.3881,  0.0602,  ...,  0.9774, -1.2408,  2.9292]]],\n",
      "       grad_fn=<AddBackward0>) 1\n"
     ]
    }
   ],
   "source": [
    "residual, batch_size = Q, Q.size(0)\n",
    "print(residual, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6816,  0.6673, -0.4468,  ..., -1.0690,  0.6963, -0.2285],\n",
      "         [ 0.7088,  0.8162,  0.3654,  ...,  0.6470,  0.3708,  0.5197],\n",
      "         [ 1.5571,  0.1647,  1.2334,  ..., -0.5666,  0.5689, -0.5585],\n",
      "         [ 1.3335,  0.4457,  0.0876,  ...,  0.5219, -0.1814,  0.5346],\n",
      "         [ 0.0571,  0.7212, -0.4121,  ..., -0.3992, -0.3169, -1.4227]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "# x1를 weight 행렬인 WQ로 곱하는 것은 현재 단어와 연관된 query 벡터인 q1를 생성합니다.\n",
    "print(W_Q(Q))\n",
    "print(W_Q(Q).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 8, 64])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 512 -> 8, 64\n",
    "# d_k = d_v = 64  # dimension of K(=Q), V\n",
    "# n_heads = 8  # number of heads in Multi-Head Attention\n",
    "W_Q(Q).view(batch_size, -1, n_heads, d_k).shape\n",
    "\n",
    "# torch.Size([1, 5, 8, 64]) -> 1 batch, 5 tokens, 8 attention heads, 64 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 64])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_s = W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "k_s = W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "v_s = W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 64])\n",
      "torch.Size([1, 8, 5, 64])\n",
      "torch.Size([1, 8, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "print(q_s.shape)\n",
    "print(k_s.shape)\n",
    "print(v_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "torch.Size([1, 1, 5, 5])\n",
      "torch.Size([1, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "print(attn_mask.shape)\n",
    "print(attn_mask.unsqueeze(1).shape)\n",
    "print(attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1).shape)\n",
    "\n",
    "attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]],\n",
       "\n",
       "         [[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]],\n",
       "\n",
       "         [[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]],\n",
       "\n",
       "         [[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]],\n",
       "\n",
       "         [[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]],\n",
       "\n",
       "         [[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]],\n",
       "\n",
       "         [[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]],\n",
       "\n",
       "         [[0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1],\n",
       "          [0, 0, 0, 0, 1]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask \n",
    "# attn_mask : [batch_size x n_heads x len_q x len_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 64])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 5])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_s.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 5])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(q_s, k_s.transpose(-1, -2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(d_k)\n",
    "scores = torch.matmul(q_s, k_s.transpose(-1, -2)) / np.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-5.9383e-01, -2.4807e-01,  6.5411e-01, -1.1670e-01, -7.2737e-01],\n",
       "          [-9.3032e-01, -4.2527e-01,  4.5711e-01, -2.3553e-01, -4.9635e-01],\n",
       "          [-1.6991e-01,  2.3219e-01,  5.1079e-01,  2.6412e-01, -5.9693e-01],\n",
       "          [-1.9371e-01,  1.4250e+00, -3.6556e-01, -1.8922e-01,  2.2626e-01],\n",
       "          [-9.7784e-01, -6.4192e-01, -3.1446e-01, -9.6350e-01, -1.0996e+00]],\n",
       "\n",
       "         [[-3.0698e-01, -9.2977e-01,  5.2781e-01,  1.8117e-01,  7.2998e-02],\n",
       "          [-4.8617e-01, -5.0216e-01, -2.7823e-01, -2.1277e-01, -4.1139e-01],\n",
       "          [ 6.6586e-02, -8.6792e-01, -6.5248e-01,  4.0352e-01, -1.4613e-01],\n",
       "          [ 3.2779e-01,  7.2557e-01, -5.0681e-02,  4.2920e-01,  2.7464e-01],\n",
       "          [-3.8958e-01, -5.4248e-01, -1.4609e-01, -2.3969e-01, -5.1014e-01]],\n",
       "\n",
       "         [[-3.2309e-02,  4.2039e-01, -2.9104e-01,  6.6914e-01, -4.7894e-01],\n",
       "          [ 3.9633e-01,  8.9111e-01,  6.4468e-01,  9.0339e-01,  1.8073e-01],\n",
       "          [-3.1823e-01, -7.3168e-02,  7.1581e-02, -1.6773e-02, -6.5338e-01],\n",
       "          [ 3.2842e-01,  1.7158e-01,  2.2579e-01, -4.2407e-01,  3.7224e-01],\n",
       "          [-5.1685e-01, -6.3382e-03, -3.8632e-01,  4.0979e-01, -2.2621e-02]],\n",
       "\n",
       "         [[ 7.5803e-01,  3.8003e-02, -8.3694e-02, -1.0016e-01,  5.3382e-01],\n",
       "          [-1.0359e-01,  9.3404e-01,  3.7414e-01,  2.1801e-01,  9.8149e-01],\n",
       "          [ 6.4451e-01,  1.1687e-01, -1.2408e-01,  2.5848e-01,  6.3129e-01],\n",
       "          [ 1.0057e+00,  1.4566e-01,  3.7256e-01,  4.8795e-01,  1.8193e+00],\n",
       "          [ 2.9491e-01,  3.2448e-01, -5.4045e-01,  8.2615e-02,  7.3022e-01]],\n",
       "\n",
       "         [[ 8.7865e-01,  4.0168e-01,  3.3414e-01, -8.6907e-02,  2.0095e-01],\n",
       "          [ 6.7654e-01,  2.0515e-01, -1.7826e-01, -2.1775e-01,  7.6356e-01],\n",
       "          [-1.0997e-01,  1.7129e-01,  1.6745e-01,  1.2118e-01,  4.8886e-02],\n",
       "          [ 2.3715e-01,  1.9528e-01, -3.7190e-01,  8.6139e-01,  4.0119e-01],\n",
       "          [-1.5039e-01,  5.6224e-01,  3.7135e-01,  3.9591e-01,  7.0662e-01]],\n",
       "\n",
       "         [[-3.4470e-01,  6.0308e-01, -1.9878e-01,  5.7386e-01,  3.0655e-01],\n",
       "          [-1.8713e-01,  8.4904e-01, -2.1044e-03,  1.1429e+00,  1.2708e+00],\n",
       "          [-1.5483e-01,  6.8780e-01, -5.1136e-01,  1.0057e+00,  6.3552e-01],\n",
       "          [ 7.1660e-01,  8.2961e-01,  6.0303e-01,  7.2413e-01,  3.1891e-01],\n",
       "          [ 1.1404e-03,  5.5208e-01, -7.9966e-02,  9.7855e-01,  6.4406e-01]],\n",
       "\n",
       "         [[ 4.6218e-01, -7.0740e-02,  7.2444e-01,  6.7153e-02, -3.3251e-01],\n",
       "          [-4.1084e-02,  1.0087e-01, -1.4664e-01, -2.2376e-01, -1.1239e-01],\n",
       "          [ 2.8537e-01,  2.6471e-01,  8.7259e-01,  7.7143e-01,  3.8632e-01],\n",
       "          [ 6.5540e-01,  4.9831e-01,  1.0325e+00,  1.0873e+00,  9.1283e-01],\n",
       "          [ 1.4146e+00,  5.6488e-01,  7.5666e-01,  8.1062e-01,  6.0438e-01]],\n",
       "\n",
       "         [[ 4.9766e-01,  6.6670e-01,  3.4311e-01,  3.1442e-01,  1.9772e-01],\n",
       "          [-3.6204e-01, -8.2890e-01,  2.2646e-01, -6.8751e-01, -1.8922e-01],\n",
       "          [ 7.6723e-02, -6.8269e-01, -1.2859e-01,  2.3345e-01,  5.8550e-02],\n",
       "          [ 2.2387e-01, -1.6666e-01, -4.5156e-01,  7.9299e-01, -9.6917e-02],\n",
       "          [-9.9521e-01, -9.4697e-01, -6.6152e-01, -1.8549e-01,  6.6720e-01]]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 5])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-5.9383e-01, -2.4807e-01,  6.5411e-01, -1.1670e-01, -1.0000e+09],\n",
       "          [-9.3032e-01, -4.2527e-01,  4.5711e-01, -2.3553e-01, -1.0000e+09],\n",
       "          [-1.6991e-01,  2.3219e-01,  5.1079e-01,  2.6412e-01, -1.0000e+09],\n",
       "          [-1.9371e-01,  1.4250e+00, -3.6556e-01, -1.8922e-01, -1.0000e+09],\n",
       "          [-9.7784e-01, -6.4192e-01, -3.1446e-01, -9.6350e-01, -1.0000e+09]],\n",
       "\n",
       "         [[-3.0698e-01, -9.2977e-01,  5.2781e-01,  1.8117e-01, -1.0000e+09],\n",
       "          [-4.8617e-01, -5.0216e-01, -2.7823e-01, -2.1277e-01, -1.0000e+09],\n",
       "          [ 6.6586e-02, -8.6792e-01, -6.5248e-01,  4.0352e-01, -1.0000e+09],\n",
       "          [ 3.2779e-01,  7.2557e-01, -5.0681e-02,  4.2920e-01, -1.0000e+09],\n",
       "          [-3.8958e-01, -5.4248e-01, -1.4609e-01, -2.3969e-01, -1.0000e+09]],\n",
       "\n",
       "         [[-3.2309e-02,  4.2039e-01, -2.9104e-01,  6.6914e-01, -1.0000e+09],\n",
       "          [ 3.9633e-01,  8.9111e-01,  6.4468e-01,  9.0339e-01, -1.0000e+09],\n",
       "          [-3.1823e-01, -7.3168e-02,  7.1581e-02, -1.6773e-02, -1.0000e+09],\n",
       "          [ 3.2842e-01,  1.7158e-01,  2.2579e-01, -4.2407e-01, -1.0000e+09],\n",
       "          [-5.1685e-01, -6.3382e-03, -3.8632e-01,  4.0979e-01, -1.0000e+09]],\n",
       "\n",
       "         [[ 7.5803e-01,  3.8003e-02, -8.3694e-02, -1.0016e-01, -1.0000e+09],\n",
       "          [-1.0359e-01,  9.3404e-01,  3.7414e-01,  2.1801e-01, -1.0000e+09],\n",
       "          [ 6.4451e-01,  1.1687e-01, -1.2408e-01,  2.5848e-01, -1.0000e+09],\n",
       "          [ 1.0057e+00,  1.4566e-01,  3.7256e-01,  4.8795e-01, -1.0000e+09],\n",
       "          [ 2.9491e-01,  3.2448e-01, -5.4045e-01,  8.2615e-02, -1.0000e+09]],\n",
       "\n",
       "         [[ 8.7865e-01,  4.0168e-01,  3.3414e-01, -8.6907e-02, -1.0000e+09],\n",
       "          [ 6.7654e-01,  2.0515e-01, -1.7826e-01, -2.1775e-01, -1.0000e+09],\n",
       "          [-1.0997e-01,  1.7129e-01,  1.6745e-01,  1.2118e-01, -1.0000e+09],\n",
       "          [ 2.3715e-01,  1.9528e-01, -3.7190e-01,  8.6139e-01, -1.0000e+09],\n",
       "          [-1.5039e-01,  5.6224e-01,  3.7135e-01,  3.9591e-01, -1.0000e+09]],\n",
       "\n",
       "         [[-3.4470e-01,  6.0308e-01, -1.9878e-01,  5.7386e-01, -1.0000e+09],\n",
       "          [-1.8713e-01,  8.4904e-01, -2.1044e-03,  1.1429e+00, -1.0000e+09],\n",
       "          [-1.5483e-01,  6.8780e-01, -5.1136e-01,  1.0057e+00, -1.0000e+09],\n",
       "          [ 7.1660e-01,  8.2961e-01,  6.0303e-01,  7.2413e-01, -1.0000e+09],\n",
       "          [ 1.1404e-03,  5.5208e-01, -7.9966e-02,  9.7855e-01, -1.0000e+09]],\n",
       "\n",
       "         [[ 4.6218e-01, -7.0740e-02,  7.2444e-01,  6.7153e-02, -1.0000e+09],\n",
       "          [-4.1084e-02,  1.0087e-01, -1.4664e-01, -2.2376e-01, -1.0000e+09],\n",
       "          [ 2.8537e-01,  2.6471e-01,  8.7259e-01,  7.7143e-01, -1.0000e+09],\n",
       "          [ 6.5540e-01,  4.9831e-01,  1.0325e+00,  1.0873e+00, -1.0000e+09],\n",
       "          [ 1.4146e+00,  5.6488e-01,  7.5666e-01,  8.1062e-01, -1.0000e+09]],\n",
       "\n",
       "         [[ 4.9766e-01,  6.6670e-01,  3.4311e-01,  3.1442e-01, -1.0000e+09],\n",
       "          [-3.6204e-01, -8.2890e-01,  2.2646e-01, -6.8751e-01, -1.0000e+09],\n",
       "          [ 7.6723e-02, -6.8269e-01, -1.2859e-01,  2.3345e-01, -1.0000e+09],\n",
       "          [ 2.2387e-01, -1.6666e-01, -4.5156e-01,  7.9299e-01, -1.0000e+09],\n",
       "          [-9.9521e-01, -9.4697e-01, -6.6152e-01, -1.8549e-01, -1.0000e+09]]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill_(attn_mask, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1332, 0.1882, 0.4639, 0.2146, 0.0000],\n",
      "          [0.1154, 0.1912, 0.4622, 0.2312, 0.0000],\n",
      "          [0.1663, 0.2486, 0.3285, 0.2567, 0.0000],\n",
      "          [0.1267, 0.6394, 0.1067, 0.1273, 0.0000],\n",
      "          [0.1867, 0.2613, 0.3625, 0.1894, 0.0000]],\n",
      "\n",
      "         [[0.1828, 0.0981, 0.4213, 0.2979, 0.0000],\n",
      "          [0.2208, 0.2173, 0.2718, 0.2902, 0.0000],\n",
      "          [0.3048, 0.1197, 0.1485, 0.4269, 0.0000],\n",
      "          [0.2336, 0.3478, 0.1600, 0.2586, 0.0000],\n",
      "          [0.2328, 0.1998, 0.2970, 0.2704, 0.0000]],\n",
      "\n",
      "         [[0.1865, 0.2933, 0.1440, 0.3762, 0.0000],\n",
      "          [0.1791, 0.2938, 0.2296, 0.2974, 0.0000],\n",
      "          [0.1958, 0.2502, 0.2892, 0.2647, 0.0000],\n",
      "          [0.3097, 0.2648, 0.2795, 0.1459, 0.0000],\n",
      "          [0.1579, 0.2631, 0.1800, 0.3990, 0.0000]],\n",
      "\n",
      "         [[0.4271, 0.2079, 0.1840, 0.1810, 0.0000],\n",
      "          [0.1468, 0.4142, 0.2366, 0.2024, 0.0000],\n",
      "          [0.3658, 0.2158, 0.1696, 0.2487, 0.0000],\n",
      "          [0.3922, 0.1659, 0.2082, 0.2337, 0.0000],\n",
      "          [0.3056, 0.3148, 0.1325, 0.2471, 0.0000]],\n",
      "\n",
      "         [[0.3874, 0.2404, 0.2247, 0.1475, 0.0000],\n",
      "          [0.4068, 0.2539, 0.1730, 0.1663, 0.0000],\n",
      "          [0.2039, 0.2701, 0.2691, 0.2569, 0.0000],\n",
      "          [0.2288, 0.2195, 0.1245, 0.4272, 0.0000],\n",
      "          [0.1550, 0.3161, 0.2612, 0.2677, 0.0000]],\n",
      "\n",
      "         [[0.1381, 0.3562, 0.1598, 0.3460, 0.0000],\n",
      "          [0.1136, 0.3202, 0.1367, 0.4295, 0.0000],\n",
      "          [0.1386, 0.3219, 0.0970, 0.4424, 0.0000],\n",
      "          [0.2488, 0.2785, 0.2221, 0.2506, 0.0000],\n",
      "          [0.1584, 0.2747, 0.1460, 0.4209, 0.0000]],\n",
      "\n",
      "         [[0.2809, 0.1648, 0.3651, 0.1892, 0.0000],\n",
      "          [0.2574, 0.2966, 0.2316, 0.2144, 0.0000],\n",
      "          [0.1850, 0.1812, 0.3329, 0.3008, 0.0000],\n",
      "          [0.2061, 0.1761, 0.3005, 0.3174, 0.0000],\n",
      "          [0.4013, 0.1716, 0.2078, 0.2193, 0.0000]],\n",
      "\n",
      "         [[0.2582, 0.3057, 0.2212, 0.2149, 0.0000],\n",
      "          [0.2409, 0.1511, 0.4340, 0.1740, 0.0000],\n",
      "          [0.2897, 0.1356, 0.2359, 0.3388, 0.0000],\n",
      "          [0.2530, 0.1712, 0.1288, 0.4470, 0.0000],\n",
      "          [0.1757, 0.1843, 0.2452, 0.3948, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "attn = nn.Softmax(dim=-1)(scores)\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 5])\n",
      "torch.Size([1, 8, 5, 64])\n",
      "torch.Size([1, 8, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "print(attn.shape)\n",
    "print(v_s.shape)\n",
    "context = torch.matmul(attn, v_s) # softmax * value\n",
    "print(context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 8, 64])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 8, 64])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.transpose(1, 2).contiguous().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) \n",
    "# context: [batch_size x len_q x n_heads * d_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape # sum???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_outputs = nn.Linear(n_heads * d_v, d_model)(context) # 512 -> 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mh_outputs = nn.LayerNorm(d_model)(mh_outputs + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_mh_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "enc_pf_outputs = self.pos_ffn(enc_mh_outputs)\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return nn.LayerNorm(d_model)(output + residual)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ff = 2048 # FeedForward dimension\n",
    "conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "\n",
    "pf_residual = enc_mh_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 512, 5])\n",
      "torch.Size([1, 2048, 5])\n"
     ]
    }
   ],
   "source": [
    "print(conv1)\n",
    "print(pf_residual.shape)\n",
    "print(enc_mh_outputs.transpose(1, 2).shape)\n",
    "print(conv1(enc_mh_outputs.transpose(1, 2)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.3408, 0.0000, 0.0000, 0.2808],\n",
      "         [0.0000, 0.0681, 0.0000, 0.0000, 0.7553],\n",
      "         [0.0000, 0.0000, 0.1571, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.5268, 0.0000, 0.1336, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3500, 0.2845],\n",
      "         [0.8660, 0.4463, 0.0000, 0.6833, 0.0000]]],\n",
      "       grad_fn=<ThresholdBackward0>)\n",
      "torch.Size([1, 2048, 5])\n"
     ]
    }
   ],
   "source": [
    "pf_output = nn.ReLU()(conv1(enc_mh_outputs.transpose(1, 2)))\n",
    "print(pf_output)\n",
    "print(pf_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
      "torch.Size([1, 512, 5])\n",
      "torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "print(conv2)\n",
    "print(conv2(pf_output).shape)\n",
    "pf_output = conv2(pf_output).transpose(1, 2)\n",
    "print(pf_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LayerNorm(d_model)(pf_output + residual).shape\n",
    "# enc_self_attns = []의 0번째 element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
