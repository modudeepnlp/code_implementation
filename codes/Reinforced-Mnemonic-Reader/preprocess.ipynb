{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d99e02918cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "from util.tokenizer import normalize_text, normal_query\n",
    "from util.spacy_tokenizer import SpacyTokenizer\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from multiprocessing.util import Finalize\n",
    "from functools import partial\n",
    "TOK = None\n",
    "ANNTOTORS = {'lemma', 'pos', 'ner'}\n",
    "\n",
    "def reform_text(text):\n",
    "#     text = re.sub(u'-|¢|¥|€|£|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/', token_extend, text)\n",
    "    text = text.strip(' \\n')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "TOK = SpacyTokenizer(annotators=ANNTOTORS)\n",
    "\n",
    "def word_tokenize(text, norm=False):\n",
    "    tokens = TOK.tokenize(reform_text(text)) # reform_text\n",
    "    output = {\n",
    "        'words': tokens.words(),\n",
    "        'pos': tokens.pos(),\n",
    "        'ner': tokens.entities(),\n",
    "        'lemma': tokens.lemmas(),\n",
    "    }\n",
    "    if norm:\n",
    "        output['words'] = [normalize_text(t) for t in output['words']]\n",
    "    return output\n",
    "\n",
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter, pos_counter, ner_counter):\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    total = 0\n",
    "    unans = 0\n",
    "    ans = 0\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    # fetch contexts and questions\n",
    "    with open(filename, \"r\") as fh:\n",
    "        source = json.load(fh)\n",
    "        for article in source[\"data\"]:\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                contexts.append(para[\"context\"].replace(\"''\", '\" ').replace(\"``\", '\" '))\n",
    "                questions.append(para['qas'])\n",
    "                \n",
    "    print(\"Generating {} context...\".format(data_type))            \n",
    "    make_pool = partial(Pool, 12)\n",
    "    workers = make_pool(initargs=())\n",
    "    c_tokens = workers.map(word_tokenize, contexts)\n",
    "    print(\"Generating {} context over\".format(data_type)) \n",
    "    for i in tqdm(range(len(c_tokens))):\n",
    "        ct = c_tokens[i]\n",
    "        # get tokens, pos, ner, lemma\n",
    "        context_tokens = ct['words']\n",
    "        context_pos = ct['pos']\n",
    "        context_ner = ct['ner']\n",
    "        context_lemma = ct['lemma']\n",
    "        spans = convert_idx(contexts[i], context_tokens)\n",
    "        context_tokens = [normalize_text(t) for t in context_tokens]\n",
    "        context_chars = [list(token) for token in context_tokens]\n",
    "        for j in range(len(context_tokens)):\n",
    "            word_counter[context_tokens[j]] += len(questions[i])\n",
    "            pos_counter[context_pos[j]] += len(questions[i])\n",
    "            ner_counter[context_ner[j]] += len(questions[i])\n",
    "            for char in context_tokens[j]:\n",
    "                char_counter[char] += len(questions[i])\n",
    "        for qa in questions[i]:\n",
    "            total += 1\n",
    "            ques = qa[\"question\"].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "            qt = word_tokenize(ques, norm=True)\n",
    "            ques_tokens = qt['words']\n",
    "            # 预处理：替换question里context出现过的数字\n",
    "            ques_tokens = normal_query(ques_tokens, context_tokens)\n",
    "            ques_pos = qt['pos']\n",
    "            ques_ner = qt['ner']\n",
    "            ques_lemma = qt['lemma']\n",
    "            ques_chars = [list(token) for token in ques_tokens]\n",
    "            for j, token in enumerate(ques_tokens):\n",
    "                word_counter[token] += 1\n",
    "                pos_counter[ques_pos[j]] += 1\n",
    "                ner_counter[ques_ner[j]] += 1\n",
    "                for char in token:\n",
    "                    char_counter[char] += 1\n",
    "            y1s, y2s = [], []\n",
    "            # 2.0 plausible answers\n",
    "            y1sp, y2sp = [], []\n",
    "            answer_texts = []\n",
    "            \n",
    "            # 2.0 Dataset\n",
    "            if 'is_impossible' in qa and qa['is_impossible']==True:\n",
    "                unans += 1\n",
    "                for answer in qa[\"plausible_answers\"]:\n",
    "                    answer_text = answer[\"text\"]\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                    answer_span = []\n",
    "                    for idx, span in enumerate(spans):\n",
    "                        if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                            answer_span.append(idx)\n",
    "                    if len(answer_span)==0:\n",
    "                        print(answer,answer_text)\n",
    "                    y1, y2 = answer_span[0], answer_span[-1]\n",
    "                    y1sp.append(y1)\n",
    "                    y2sp.append(y2)\n",
    "                y1s.append(-1)\n",
    "                y2s.append(-1)\n",
    "            else:\n",
    "                ans += 1\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    answer_text = answer[\"text\"]\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                    answer_texts.append(answer_text)\n",
    "                    answer_span = []\n",
    "                    for idx, span in enumerate(spans):\n",
    "                        if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                            answer_span.append(idx)\n",
    "                    if len(answer_span)==0:\n",
    "                        print(answer,answer_text)\n",
    "#                     else:\n",
    "#                         print(answer_text, '###', np.array(context_tokens)[answer_span])\n",
    "                    y1, y2 = answer_span[0], answer_span[-1]\n",
    "                    y1s.append(y1)\n",
    "                    y2s.append(y2)\n",
    "                    y1sp.append(y1)\n",
    "                    y2sp.append(y2)\n",
    "            example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars, \n",
    "                       'context_lemma':context_lemma, 'context_pos':context_pos, 'context_ner':context_ner,\n",
    "                       \"ques_tokens\": ques_tokens, \"ques_chars\": ques_chars, \n",
    "                       'ques_lemma':ques_lemma, 'ques_pos':ques_pos, 'ques_ner':ques_ner,\n",
    "                       \"y1s\": y1s, \"y2s\": y2s, \n",
    "                       'y1sp':y1sp, 'y2sp':y2sp, \n",
    "                       \"id\": total}\n",
    "            examples.append(example)\n",
    "            eval_examples[str(total)] = {\"question\":ques,\n",
    "                                         \"context\": contexts[i], \n",
    "                                         \"spans\": spans, \n",
    "                                         \"answers\": answer_texts, \n",
    "                                         \"uuid\": qa[\"id\"]}\n",
    "    print(\"{} questions in total\".format(len(examples)))\n",
    "    print('answerable:',ans,'unanswerable:',unans)\n",
    "    \n",
    "    return examples, eval_examples\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    if data_type=='char':\n",
    "        embedding_dict_fix={}\n",
    "        embedding_dict_trainable={}\n",
    "        assert size is not None\n",
    "        assert vec_size is not None\n",
    "        assert emb_file is not None\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.split()\n",
    "                char = \"\".join(array[0:-vec_size])\n",
    "                char = normalize_text(char)\n",
    "                vector = list(map(float, array[-vec_size:]))\n",
    "                if char in counter and counter[char] > limit:\n",
    "                    embedding_dict_fix[char] = vector\n",
    "        print(\"{} / {} char tokens have corresponding {} embedding vector\".format(\n",
    "                len(embedding_dict_fix), len(filtered_elements), data_type))\n",
    "        for token in filtered_elements:\n",
    "            if token not in embedding_dict_fix:\n",
    "                embedding_dict_trainable[token] = [np.random.normal(scale=0.1) for _ in range(vec_size)]\n",
    "        \n",
    "        # trainable emb mat\n",
    "        NULL = \"--NULL--\"\n",
    "        OOV = \"--OOV--\"\n",
    "        token2idx_dict = {token: idx for idx,\n",
    "                          token in enumerate(embedding_dict_trainable.keys(), 2)}\n",
    "        token2idx_dict[NULL] = 0\n",
    "        token2idx_dict[OOV] = 1\n",
    "        embedding_dict_trainable[NULL] = [0. for _ in range(vec_size)]\n",
    "        embedding_dict_trainable[OOV] = [0. for _ in range(vec_size)] # np.random.random((vec_size))/2-0.25\n",
    "        idx2emb_dict = {idx: embedding_dict_trainable[token]\n",
    "                        for token, idx in token2idx_dict.items()}\n",
    "        emb_mat_trainable = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "        \n",
    "        # fix emb mat\n",
    "        for idx, token in enumerate(embedding_dict_fix.keys(), len(token2idx_dict)):\n",
    "            token2idx_dict[token] = idx\n",
    "        for token, idx in token2idx_dict.items():\n",
    "            if token not in embedding_dict_trainable:\n",
    "                idx2emb_dict[idx] = embedding_dict_fix[token]\n",
    "        emb_mat_fix = [idx2emb_dict[idx] for idx in range(len(emb_mat_trainable), len(idx2emb_dict))]\n",
    "        print('idx2emb_dict:',len(idx2emb_dict))\n",
    "        print('token2idx_dict:',len(token2idx_dict))\n",
    "        print('emb_mat_trainable:',len(emb_mat_trainable))\n",
    "        print('emb_mat_fix:',len(emb_mat_fix))\n",
    "        return (emb_mat_trainable, emb_mat_fix), token2idx_dict, idx2emb_dict\n",
    "    else:\n",
    "        embedding_dict={}\n",
    "        if emb_file is not None:\n",
    "            assert size is not None\n",
    "            assert vec_size is not None\n",
    "            with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "                for line in tqdm(fh, total=size):\n",
    "                    array = line.split()\n",
    "                    word = \"\".join(array[0:-vec_size])\n",
    "                    word = normalize_text(word)\n",
    "                    vector = list(map(float, array[-vec_size:]))\n",
    "                    if word in counter and counter[word] > limit:\n",
    "                        embedding_dict[word] = vector\n",
    "            print(\"{} / {} word tokens have corresponding {} embedding vector\".format(\n",
    "                len(embedding_dict), len(filtered_elements), data_type))\n",
    "        else:\n",
    "            assert vec_size is not None\n",
    "            for token in filtered_elements:\n",
    "                embedding_dict[token] = [np.random.normal(scale=0.1) for _ in range(vec_size)]\n",
    "            print(\"{} char tokens have corresponding embedding vector\".format(\n",
    "                len(filtered_elements)))\n",
    "\n",
    "        NULL = \"--NULL--\"\n",
    "        OOV = \"--OOV--\"\n",
    "        token2idx_dict = {token: idx for idx,\n",
    "                          token in enumerate(embedding_dict.keys(), 2)}\n",
    "        token2idx_dict[NULL] = 0\n",
    "        token2idx_dict[OOV] = 1\n",
    "        embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "        embedding_dict[OOV] = [0. for _ in range(vec_size)] # np.random.random((vec_size))/2-0.25\n",
    "        idx2emb_dict = {idx: embedding_dict[token]\n",
    "                        for token, idx in token2idx_dict.items()}\n",
    "        emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "        return emb_mat, token2idx_dict, idx2emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "\n",
    "# # 2.0 Dataset\n",
    "# test_examples, test_eval = process_file('original_data/dev-v2.0.json', \"test\", word_counter, char_counter)\n",
    "# train_examples, train_eval = process_file('original_data/train-v2.0.json', \"train\", word_counter, char_counter)\n",
    "\n",
    "# 1.0 Dataset\n",
    "train_examples, train_eval = process_file('../../fwei/data/squad/train-v1.1.json', \"train\", word_counter, char_counter)\n",
    "# dev_examples, dev_eval = process_file('../../fwei/data/squad/dev-v1.2.json', \"dev\", word_counter, char_counter)\n",
    "test_examples, test_eval = process_file('../../fwei/data/squad/dev-v1.1.json', \"test\", word_counter, char_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval\n",
    "# # 2.0 Dataset\n",
    "# with open('dataset/train_eval.json', \"w\") as fh:\n",
    "#     json.dump(train_eval, fh)\n",
    "# with open('dataset/test_eval.json','w') as fh:\n",
    "#     json.dump(test_eval,fh)\n",
    "    \n",
    "# 1.0 Dataset\n",
    "with open('dataset1.0/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "# with open('dataset1.0/dev_eval.json','w') as fh:\n",
    "#     json.dump(dev_eval,fh)\n",
    "with open('dataset1.0/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "word_counter, char_counter = Counter(), Counter()\n",
    "pos_counter, ner_counter = Counter(), Counter()\n",
    "# 2.0 Dataset\n",
    "test_examples, test_eval = process_file('original_data/dev-v1.1.json', \"test\", \n",
    "                                        word_counter, char_counter, pos_counter, ner_counter)\n",
    "train_examples, train_eval = process_file('original_data/train-v1.1.json', \"train\",\n",
    "                                          word_counter, char_counter, pos_counter, ner_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word_tokens\n",
    "all_tokens=set(list(word_counter.keys()))\n",
    "print('token num:', len(all_tokens))\n",
    "assert '<S>' not in all_tokens and '</S>' not in all_tokens\n",
    "all_tokens.add('<S>')\n",
    "all_tokens.add('</S>')\n",
    "print('token + <S> + </S> num:', len(all_tokens))\n",
    "for a in all_tokens:\n",
    "    if a=='':\n",
    "        print('nono')\n",
    "vocab_file = '../RMR_tf/dataset2/vocab.txt'\n",
    "with open(vocab_file, 'w',encoding='utf8') as fout:\n",
    "    fout.write('\\n'.join(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_eval and dev_eval\n",
    "# 2.0 Dataset\n",
    "with open('../RMR_tf/dataset2/train_eval.json', \"w\") as fh:\n",
    "    json.dump(train_eval, fh)\n",
    "with open('../RMR_tf/dataset2/test_eval.json','w') as fh:\n",
    "    json.dump(test_eval,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v 90977 91586\n",
    "(char_embmat_trainable, char_embmat_fix), char2idx_dict, _ = get_embedding(\n",
    "    char_counter, \"char\", emb_file='original_data/glove.840B.300d-char.txt', size=95, vec_size=300)\n",
    "word_emb_mat, word2idx_dict, _ = get_embedding(\n",
    "    word_counter, \"word\", emb_file='original_data/glove.840B.300d.txt', size=int(2.2e6), vec_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pos and ner embedding\n",
    "def get_tag_emb(counter):\n",
    "    emb_dict={}\n",
    "    max_len=len(counter)\n",
    "    for i,c in enumerate(counter):\n",
    "        emb_vec = np.zeros(max_len)\n",
    "        emb_vec[i]=1\n",
    "        emb_dict[c]=emb_vec\n",
    "    print('emb_dict size:',len(emb_dict))\n",
    "    return emb_dict\n",
    "\n",
    "pos_emb=get_tag_emb(pos_counter)\n",
    "ner_emb=get_tag_emb(ner_counter)\n",
    "print('all pos:',pos_counter.keys())\n",
    "print('all ner:',ner_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import h5py\n",
    "def build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, id2word_dict, \\\n",
    "                   pos_emb, ner_emb, is_test=False):\n",
    "\n",
    "    para_limit = config['test_para_limit'] if is_test else config['para_limit']\n",
    "    ques_limit = config['test_ques_limit'] if is_test else config['ques_limit']\n",
    "    ans_limit = 100 if is_test else config['ans_limit']\n",
    "    char_limit = config['char_limit']\n",
    "    \n",
    "    def match_func(question, context, question_lemma, context_lemma):\n",
    "        counter = Counter(w.lower() for w in context)\n",
    "        total = sum(counter.values())\n",
    "        freq = [counter[w.lower()] / total for w in context]\n",
    "        question_word = {w for w in question}\n",
    "        question_lower = {w.lower() for w in question}\n",
    "        question_lemma = {w if w != '-PRON-' else w.lower() for w in question_lemma}\n",
    "        match_origin = [1 if w in question_word else 0 for w in context]\n",
    "        match_lower = [1 if w.lower() in question_lower else 0 for w in context]\n",
    "        match_lemma = [1 if (w if w != '-PRON-' else w.lower()) in question_lemma else 0 for w in context_lemma]\n",
    "        features = np.asarray([freq, match_origin, match_lower, match_lemma], dtype=np.float32).T\n",
    "        return features\n",
    "\n",
    "    def filter_func(example, is_test=False):\n",
    "        if len(example['y2s'])==0 or len(example['y1s'])==0:\n",
    "            print(example)\n",
    "        return len(example[\"context_tokens\"]) > para_limit or \\\n",
    "               len(example[\"ques_tokens\"]) > ques_limit or \\\n",
    "               (example[\"y2s\"][0] - example[\"y1s\"][0]) > ans_limit\n",
    "    \n",
    "    def _get_word(word):\n",
    "        for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "            if each in word2idx_dict:\n",
    "                return word2idx_dict[each]\n",
    "        return 1\n",
    "\n",
    "    def _get_char(char):\n",
    "        if char in char2idx_dict:\n",
    "            return char2idx_dict[char]\n",
    "        return 1\n",
    "\n",
    "    print(\"Processing {} examples...\".format(data_type))\n",
    "    total = 0\n",
    "    total_ = 0\n",
    "    qids=[]\n",
    "    context_strings_all=[]\n",
    "    ques_strings_all=[]\n",
    "    unans=0\n",
    "    with h5py.File(out_file+data_type+'_data.h5','w') as h5f:\n",
    "        for example in tqdm(examples):\n",
    "            total_ += 1\n",
    "\n",
    "            if filter_func(example, is_test):\n",
    "                continue\n",
    "\n",
    "            total += 1\n",
    "            qids.append(str(example['id']))\n",
    "            c_len = len(example['context_tokens'])\n",
    "            q_len = len(example['ques_tokens'])\n",
    "            context_idxs = []\n",
    "            context_char_idxs = np.zeros([c_len, char_limit], dtype=np.int32)\n",
    "            ques_idxs = []\n",
    "            ques_char_idxs = np.zeros([q_len, char_limit], dtype=np.int32)\n",
    "            context_strings = []\n",
    "            ques_strings = []\n",
    "            context_pos = []\n",
    "            context_ner = []\n",
    "            ques_pos = []\n",
    "            ques_ner = []\n",
    "\n",
    "            if config['data_ver']==2:\n",
    "                y1 = np.zeros([c_len+1], dtype=np.float32)\n",
    "                y2 = np.zeros([c_len+1], dtype=np.float32)\n",
    "                y1p = np.zeros([c_len], dtype=np.float32)\n",
    "                y2p = np.zeros([c_len], dtype=np.float32)\n",
    "            else:\n",
    "                y1 = np.zeros([c_len], dtype=np.float32)\n",
    "                y2 = np.zeros([c_len], dtype=np.float32)\n",
    "                y1p = None\n",
    "                y2p = None\n",
    "\n",
    "            for i, token in enumerate(example[\"context_tokens\"]):\n",
    "                context_idxs.append(_get_word(token))\n",
    "                context_strings.append(token)\n",
    "            context_idxs=np.array(context_idxs)\n",
    "\n",
    "            for i, token in enumerate(example[\"ques_tokens\"]):\n",
    "                ques_idxs.append(_get_word(token)) \n",
    "                ques_strings.append(token)\n",
    "            ques_idxs=np.array(ques_idxs)\n",
    "\n",
    "            for i, token in enumerate(example[\"context_chars\"]):\n",
    "                for j, char in enumerate(token):\n",
    "                    if j == char_limit:\n",
    "                        break\n",
    "                    context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "            for i, token in enumerate(example[\"ques_chars\"]):\n",
    "                for j, char in enumerate(token):\n",
    "                    if j == char_limit:\n",
    "                        break\n",
    "                    ques_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "            for i, token in enumerate(example[\"context_pos\"]):\n",
    "                context_pos.append(pos_emb[token])\n",
    "            context_pos=np.array(context_pos)\n",
    "            \n",
    "            for i, token in enumerate(example[\"context_ner\"]):\n",
    "                context_ner.append(ner_emb[token])\n",
    "            context_ner=np.array(context_ner)\n",
    "            \n",
    "            context_match = match_func(example[\"ques_tokens\"], example[\"context_tokens\"], \n",
    "                                       example[\"ques_lemma\"], example[\"context_lemma\"])\n",
    "            context_feat = np.concatenate([context_pos, context_ner, context_match], axis=-1) # [c_len, 50+18+4]\n",
    "\n",
    "            for i, token in enumerate(example[\"ques_pos\"]):\n",
    "                ques_pos.append(pos_emb[token])\n",
    "            ques_pos=np.array(ques_pos)\n",
    "            \n",
    "            for i, token in enumerate(example[\"ques_ner\"]):\n",
    "                ques_ner.append(ner_emb[token])\n",
    "            ques_ner=np.array(ques_ner)\n",
    "            \n",
    "            ques_match = match_func(example[\"context_tokens\"], example[\"ques_tokens\"], \n",
    "                                          example[\"context_lemma\"], example[\"ques_lemma\"])\n",
    "            ques_feat = np.concatenate([ques_pos, ques_ner, ques_match], axis=-1) # [q_len, 50+18+4]\n",
    "\n",
    "\n",
    "            start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
    "            if config['data_ver']==2: \n",
    "                if len(example[\"y1sp\"])!=0:\n",
    "                    startp, endp = example[\"y1sp\"][-1], example[\"y2sp\"][-1]\n",
    "                if start!=-1 and end!=-1:\n",
    "                    y1[start+1], y2[end+1] = 1.0, 1.0\n",
    "                    y1p[start], y2p[end] = 1.0, 1.0\n",
    "                else:\n",
    "                    y1[0], y2[0] = 1.0, 1.0\n",
    "                    if len(example[\"y1sp\"])!=0:\n",
    "                        y1p[startp], y2p[endp] = 1.0, 1.0\n",
    "                    unans+=1\n",
    "            else:\n",
    "                y1[start], y2[end] = 1.0, 1.0\n",
    "            \n",
    "            data_simple = h5f.create_group(str(example['id']))\n",
    "            data_simple.create_dataset('context_ids', data = context_idxs)\n",
    "            data_simple.create_dataset('ques_ids', data = ques_idxs)\n",
    "            data_simple.create_dataset('context_char_ids', data = context_char_idxs)\n",
    "            data_simple.create_dataset('ques_char_ids', data = ques_char_idxs)\n",
    "            data_simple.create_dataset('y1', data = y1)\n",
    "            data_simple.create_dataset('y2', data = y2)\n",
    "            if config['data_ver']==2:\n",
    "                data_simple.create_dataset('y1p', data = y1p)\n",
    "                data_simple.create_dataset('y2p', data = y2p)\n",
    "            data_simple.create_dataset('context_feat', data = context_feat)\n",
    "            data_simple.create_dataset('ques_feat', data = ques_feat)\n",
    "            context_strings_all.append(context_strings)\n",
    "            ques_strings_all.append(ques_strings)\n",
    "            \n",
    "    with open(out_file+data_type+'_contw_strings.pkl','wb') as f:\n",
    "        pickle.dump(context_strings_all, f)\n",
    "    with open(out_file+data_type+'_quesw_strings.pkl','wb') as f:\n",
    "        pickle.dump(ques_strings_all, f)\n",
    "        \n",
    "    np.save(out_file+data_type+'_qid.npy',qids)\n",
    "    \n",
    "    print(\"Built {} / {} instances of features in total\".format(total, total_))\n",
    "    print('unanswerable:',unans)\n",
    "\n",
    "config={\n",
    "    'test_para_limit':1000,\n",
    "    'test_ques_limit':50,\n",
    "    'para_limit':400,\n",
    "    'ques_limit':50,\n",
    "    'ans_limit':30,\n",
    "    'char_limit':16,\n",
    "    'data_ver':1,\n",
    "    'typo_correct':True\n",
    "}\n",
    "\n",
    "# 2.0 Dataset\n",
    "build_features(config, train_examples, 'train', '../RMR_tf/dataset2/', word2idx_dict, char2idx_dict, id2word_dict, \n",
    "               pos_emb, ner_emb, is_test=False)\n",
    "build_features(config, test_examples, 'dev', '../RMR_tf/dataset2/', word2idx_dict, char2idx_dict, id2word_dict, \n",
    "               pos_emb, ner_emb, is_test=False)\n",
    "# build_features(config, test_examples, 'test', 'dataset_pre3/', word2idx_dict, char2idx_dict, id2word_dict, \n",
    "#                pos_emb, ner_emb, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_type='dev'\n",
    "cont_string=np.load(os.path.join('dataset',data_type+'_contw_strings.npy'))\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "print([i.tag_ for i in nlp('cannot')])\n",
    "# words=cont_string[0,:]\n",
    "# print(x)\n",
    "# tags_=[]\n",
    "# for w in words:\n",
    "#     wtag=[j.tag_ for j in nlp(str(w))]\n",
    "#     tags_.append(wtag)\n",
    "# print(words)\n",
    "# print(tags_)\n",
    "nlp = spacy.load(\"en\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def gettag(cont_string):\n",
    "    contexts=[]\n",
    "    wrong_num=0\n",
    "    for i in tqdm(range(cont_string.shape[0])):#range(cont_string.shape[0])\n",
    "        sentences=[]\n",
    "        words=[]\n",
    "        for j in range(cont_string.shape[1]):\n",
    "            if cont_string[i,j]=='':\n",
    "                break\n",
    "\n",
    "            # 规则矫正：\n",
    "            # 1.如果只有一个'，去除\n",
    "            if str(cont_string[i,j]).count('\\'')==1 and len(cont_string[i,j])>1:\n",
    "                cont_string[i,j]=cont_string[i,j].replace('\\'','')\n",
    "                \n",
    "            # 2.如果如果是cannot，改为not\n",
    "            if str(cont_string[i,j]).lower()=='cannot':\n",
    "                cont_string[i,j]='not'\n",
    "                \n",
    "            # 3.im，改为I\n",
    "            if str(cont_string[i,j]).lower()=='im':\n",
    "                cont_string[i,j]='i'\n",
    "                \n",
    "            # # 其余问题过滤（暂时）128K\n",
    "            # if str(cont_string[i,j])=='128K':\n",
    "            #     cont_string[i,j]='128'\n",
    "                \n",
    "            words.append(cont_string[i,j])\n",
    "            if words[-1]=='.' or words[-1]=='!' or words[-1]=='?':\n",
    "                sentence=' '.join(words)\n",
    "                tags=[n.tag_ for n in nlp(sentence)]\n",
    "                if len(tags)!=len(words):\n",
    "                    tags_=[]\n",
    "                    for w in words:\n",
    "                        wtag=[j.tag_ for j in nlp(str(w))]\n",
    "                        if len(wtag)>1:\n",
    "                            wtag=wtag[0]\n",
    "                        tags_.extend(wtag)\n",
    "                    tags=tags_\n",
    "                    wrong_num+=1\n",
    "                    assert len(tags)==len(words)\n",
    "                sentences.append(list(zip(tags,words)))\n",
    "                words=[]\n",
    "        if len(words)>0:\n",
    "            sentence=' '.join(words)\n",
    "            tags=[n.tag_ for n in nlp(sentence)]\n",
    "            if len(tags)!=len(words):\n",
    "                tags_=[]\n",
    "                for w in words:\n",
    "                    wtag=[j.tag_ for j in nlp(str(w))]\n",
    "                    if len(wtag)>1:\n",
    "                        wtag=wtag[0]\n",
    "                    tags_.extend(wtag)\n",
    "                tags=tags_\n",
    "                wrong_num+=1\n",
    "                assert len(tags)==len(words)\n",
    "            sentences.append(list(zip(tags,words)))\n",
    "            words=[]\n",
    "        contexts.append(sentences)\n",
    "    print(wrong_num)\n",
    "    \n",
    "    return contexts\n",
    "# contexts=gettag(cont_string)\n",
    "\n",
    "split_num=8\n",
    "temp_len=cont_string.shape[0]//split_num\n",
    "params=[]\n",
    "for i in range(split_num):\n",
    "    if i != split_num-1:\n",
    "        params.append(cont_string[i*temp_len:(i+1)*temp_len,::])\n",
    "    else:\n",
    "        params.append(cont_string[i*temp_len:,::])\n",
    "    \n",
    "from multiprocessing import Pool\n",
    "pool=Pool()\n",
    "result=[]\n",
    "for i in params:\n",
    "    result.append(pool.apply_async(gettag, kwds={'cont_string':i}))\n",
    "pool.close()\n",
    "pool.join()\n",
    "contexts=[]\n",
    "[contexts.extend(i.get()) for i in result]\n",
    "import torch\n",
    "import parse_nk\n",
    "torch.cuda.set_device(3)\n",
    "def torch_load(load_path):\n",
    "    if parse_nk.use_cuda:\n",
    "        return torch.load(load_path)\n",
    "    else:\n",
    "        return torch.load(load_path, map_location=lambda storage, location: storage)\n",
    "info = torch_load('parsing/models/en_elmo_dev.95.21.pt')\n",
    "assert 'hparams' in info['spec'], \"Older savefiles not supported\"\n",
    "info['spec']['hparams']['sentence_max_len']=400\n",
    "print(info['spec']['hparams'])\n",
    "parser = parse_nk.NKChartParser.from_spec(info['spec'], info['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('parsing/data/dev_tags.pkl','rb') as f:\n",
    "    tags=pickle.load(f)\n",
    "    tags=np.array(tags)\n",
    "tags_temp=tags[64:96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_parse_feat(tags_temp):\n",
    "    batch_size = len(tags_temp)\n",
    "    \n",
    "    # stastic the word num in each sample\n",
    "    sen_len=[sum([len(tt) for tt in t]) for t in tags_temp]\n",
    "    max_len=max(sen_len)\n",
    "    \n",
    "    # combine the sentences to a batch\n",
    "    tags_temp_new=[]\n",
    "    for i in range(len(tags_temp)):\n",
    "        combined_context=[]\n",
    "        [combined_context.extend(t) for t in tags_temp[i]]\n",
    "        tags_temp_new.append(combined_context)\n",
    "    print(len(tags_temp_new[11]))\n",
    "    # inference the parsing feature\n",
    "    feat,idxs = parser.parse_batch(tags_temp_new)\n",
    "    \n",
    "    # remove the elmo useless token from feat\n",
    "    inds=[]\n",
    "    for j in range(len(idxs.batch_idxs_np)):\n",
    "        if j==0 or j==len(idxs.batch_idxs_np)-1 or \\\n",
    "        idxs.batch_idxs_np[j-1]!=idxs.batch_idxs_np[j] or \\\n",
    "        idxs.batch_idxs_np[j+1]!=idxs.batch_idxs_np[j]:\n",
    "            continue\n",
    "        else:\n",
    "            inds.append(j)\n",
    "    feat=feat[inds,:]\n",
    "    \n",
    "    # convert feat to (batch_size, max_len, 1024)\n",
    "    assert sum(sen_len)==feat.shape[0]\n",
    "    feats=np.zeros((batch_size, max_len, 1024))\n",
    "    cusum=0\n",
    "    for i,s in enumerate(sen_len):\n",
    "        feats[i,0:s,:]=feat[cusum:cusum+s,:]\n",
    "        cusum+=s\n",
    "    assert cusum==feat.shape[0]\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "words = word_tokenize(\"im a footman.\")\n",
    "\n",
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
