{
  "attention": "normed_bahdanau",
  "attention_architecture": "gnmt_v2",
  "batch_size": 64,
  "colocate_gradients_with_ops": false,
  "dropout": 0.0,
  "encoder_type": "gnmt",
  "eos": "</s>",
  "forget_bias": 1.0,
  "infer_batch_size": 32,
  "init_weight": 0.1,
  "learning_rate": 0.01,
  "max_gradient_norm": 1.0,
  "metrics": ["f1"],
  "num_buckets": 1,
  "num_encoder_layers": 2,
  "num_decoder_layers": 4,
  "num_train_steps": 10000000,
  "num_embeddings_partitions": 2,
  "decay_scheme": "luong10",
  "num_units": 512,
  "optimizer": "sgd",
  "residual": true,
  "share_vocab": true,
  "subword_option": "spm",
  "sos": "<s>",
  "src_max_len": 50,
  "src_max_len_infer": 50,
  "steps_per_external_eval": 100,
  "steps_per_stats": 100,
  "steps_per_save": 100,
  "tgt_max_len": 50,
  "tgt_max_len_infer": 50,
  "unit_type": "lstm",
  "beam_width": 20,
  "length_penalty_weight": 1.0,
  "entropy_regularization_weight": 0.0001,
  "use_rl": true,
  "src": "source",
  "tgt": "target",
  "subword_model" : "data/spm2/spm.unigram.16k.model",
  "vocab_prefix": "data/spm2/spm.unigram.16k.vocab.nocount.notab",
  "sampling_temperature": 1.0,
  "server_mode": false,
  "baseline_type": 2
}

